<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>angel's blog - Angel C. Hernandez</title><link>/</link><description>My name is Angel C. Hernandez and I am a graduate student at Carnegie Mellon University focusing my studies in machine learning. I am also a recipient of the Science Mathematics and Research for Transformation (SMART) Fellowship, where upon graduation I will be an engineer for the army adhereing to initiatives in warfare simulation modeling at the TRADOC Analysis Center. I have found machine learning blog posts to be an excellent resource during my academic studies and I hope mine can be a benefit to you.</description><lastBuildDate>Tue, 01 Jan 2019 00:00:00 -0500</lastBuildDate><item><title>Variational Autoencoders</title><link>/test-post.html</link><description>&lt;p&gt;&lt;link rel="stylesheet" 
type="text/css" 
href="/theme/css/md-style.css"&gt;&lt;/p&gt;
&lt;h1 class="head"&gt;1. Introduction &lt;/h1&gt;

&lt;p class="body"&gt;

Like all 
&lt;a href="https://openai.com/blog/generative-models/" style="color:#FF7D33"&gt;
generative models&lt;/a&gt;, VAEs are concerned with learning a valid probability distribution from which our dataset was &lt;i&gt;generated&lt;/i&gt; from. They do this by learning a set of latent variables, 

&lt;span class="math"&gt;\(\boldsymbol{z}\)&lt;/span&gt;,


which are essentially the underlining features that best explain our observed data,

&lt;span class = "math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;.

We can then condition on these variables and sample from the distribution, 
&lt;span class = "math"&gt;
\( 
\boldsymbol{x}_{new} \sim 
p(\boldsymbol{x}_{observed}|\boldsymbol{z}) \)&lt;/span&gt;, 

to generate a new data point. This distribution happens to be tractable 
but using the posterior,

&lt;span class = "math"&gt;
\( 
\boldsymbol{z} \sim 
p(\boldsymbol{z}|\boldsymbol{x}_{observed}) \)&lt;/span&gt;,

is the distribution that has plagued machine learning since the beginning of time.
Furthermore, it is challenging to learn the parameters of both distributions in an end-to-end fashion. &lt;a href='#wakeSleep' id='ref-wakeSleep-1'&gt;(Hinton et al., 1995)&lt;/a&gt; developed the Wake-sleep algorithm which was able to learn all parameters to this exact problem via two objective functions and back-propagation. While this algorithm &lt;i&gt; masked &lt;/i&gt; the problem at hand, even Geoffrey Hinton 
&lt;a href="https://www.youtube.com/watch?v=VKpc_z7b9I0" style="color:#FF7D33"&gt; 
acknowledged&lt;/a&gt;, and I quote, &lt;i&gt; "the basic idea behind these variational methods sounds crazy." &lt;/i&gt; Luckily, &lt;a href='#vae' id='ref-vae-1'&gt;(Diederik and Max, 2014)&lt;/a&gt; offered a better solution using the &lt;i&gt;
reparameterization trick &lt;/i&gt; and deep generative models have really been in resurgence since! 

&lt;/p&gt;

&lt;h1 class="head"&gt; 2. Problem Scenario &lt;/h1&gt;

&lt;p class="body"&gt;

Let us begin by assuming two layers of latent variables where
&lt;span class="math"&gt;
\(\boldsymbol{z}=\{\boldsymbol{z}^1,\boldsymbol{z}^2\}\)&lt;/span&gt;.
As a result, we have the following graphical model:

&lt;/p&gt;

&lt;figure&gt;
  &lt;img class="fig" src="/images/vae/pgm.png" width="50%"&gt;
  &lt;figcaption class="figcaption"&gt; 
  Fig 1. Graphical model in concern 
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/p&gt;

&lt;p class="body"&gt;

Our goal is to use maximum likelihood estimation to learn the parameters, 
&lt;span class="math"&gt;\(\boldsymbol{\theta}\)&lt;/span&gt;, of the following distribution:

&lt;/p&gt;

&lt;div class="math"&gt;
&lt;p class="formula"&gt;
$$
p(\boldsymbol{x}) =  \int p_{\boldsymbol{\theta}}(\boldsymbol{x|z})
p_{\boldsymbol{\theta}}(\boldsymbol{z}) \ d\boldsymbol{z}
$$
&lt;/p&gt;
&lt;/div&gt;

&lt;p class = "body"&gt;

There are some underlining takeaways from the above distribution:

&lt;/p&gt;

&lt;ul class="body"&gt;

&lt;li&gt; It tends to be intractable because our model assumes 

&lt;span class="math"&gt; \(z_i^k \not\!\perp\!\!\!\perp z_j^k | \ \boldsymbol{x} \ \cup \
\boldsymbol{z^{k-1}} \ 
\forall \ i \neq j,k \) &lt;/span&gt; 

&lt;a href"https://ermongroup.github.io/cs228-notes/representation/directed/" style="color:#FF7D33"&gt;
(explaining away)&lt;/a&gt;.
This requires integration over &lt;b&gt;all&lt;/b&gt; combinations our latent variables can take on.
&lt;/li&gt;

&lt;li&gt; The probability density functions on the right can be the output of a neural network (assuming it returns a valid probability distribution) or we can use a neural network to generate the parameters of a more traditional distribution, say a Gaussian (which is a frequently used approach). &lt;/li&gt;
&lt;/ul&gt;

&lt;p class="body"&gt;
Since our current distribution is intractable, we must resort to variational inference to learn

&lt;span class = "math"&gt;\( \boldsymbol{\theta} \).&lt;/span&gt;

&lt;/p&gt;

&lt;h1 class = "head"&gt; 3. Variational Inference &lt;/h1&gt;

&lt;p class="body"&gt;

If we recall, MLE is a simple optimization problem which returns a set of parameters that maximizes the likelihood that our data came from some distribution of our choice. Our objective function is the log likelihood and variational inference simply replaces this function with a lower bound function (evidence lower bound or ELBO) that is  &lt;i&gt;tractable&lt;/i&gt;. So, as we learn a set of parameters that maximizes the likelihood of our lower bound we are also increasing the likelihood of our original distribution (this is the definition of a lower bound).
&lt;/p&gt;

&lt;h2 class="subhead"&gt; 3.1 Deriving the ELBO &lt;/h2&gt;

&lt;p class="body"&gt;

The derivation of the lower bound is a mere byproduct of  
&lt;a url = "https://www.probabilitycourse.com/chapter6/6_2_5_jensen's_inequality.php" style="color:#FF7D33"&gt;Jensen's Inequality&lt;/a&gt;

which simply states: for any concave function, 

&lt;span class = "math"&gt;\(g,\) &lt;/span&gt;
we have &lt;span class="math"&gt; \( g(\mathbb{E}[u]) \geq \mathbb{E}[g(u)]\). &lt;/span&gt; 

So, we layout the following:
&lt;/p&gt;

&lt;div class="math"&gt;
&lt;p class="formula"&gt;

$$
\begin{aligned}
    \log p(\boldsymbol{x}) &amp;= \log \int p_{\boldsymbol{\theta}}(\boldsymbol{x|z})
    p_{\boldsymbol{\theta}}(\boldsymbol{z}) \ d\boldsymbol{z} \\
    &amp;= \log \int p_{\boldsymbol{\theta}}(\boldsymbol{x},\boldsymbol{z}) \ d\boldsymbol{z} \\
    &amp;= \log \int q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})
    \frac{p_{\boldsymbol{\theta}}(\boldsymbol{x},\boldsymbol{z})}
    {q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})} \ d\boldsymbol{z}
\end{aligned}
$$

&lt;/p&gt;
&lt;/div&gt;

&lt;p class="body"&gt;

Ignoring 

&lt;span class="math"&gt;\(q_{\boldsymbol{\phi}} \)&lt;/span&gt; 

for a second, all we have done thus far is rewritten our log likelihood function in a &lt;i&gt;clever&lt;/i&gt; way. I emphasis clever because we can now invoke Jensen's Inequality where:

&lt;/p&gt;

&lt;div class="math"&gt;
&lt;p class="formula"&gt;
$$
\begin{aligned}
    u &amp;= \frac{p_{\boldsymbol{\theta}}(\boldsymbol{x},\boldsymbol{z})}
    {q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})}  \\
    \mathbb{E}_{\boldsymbol{z}\sim q_{\boldsymbol{\phi}}(\boldsymbol{z|x})} &amp;=
    \int q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x}) \ d\boldsymbol{z} \\
    g(\cdot) &amp;= \log(\cdot) \\
     \log p(\boldsymbol{x}) &amp;= \log 
     \mathbb{E}_{\boldsymbol{z}\sim q_{\boldsymbol{\phi}}(\boldsymbol{z|x})}
     \bigg[ \frac{p_{\boldsymbol{\theta}}(\boldsymbol{x},\boldsymbol{z})}
     {q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})}\bigg] \\
     &amp;\geq  \mathbb{E}_{\boldsymbol{z}\sim q_{\boldsymbol{\phi}}(\boldsymbol{z|x})}
     \bigg[\log 
     \frac{p_{\boldsymbol{\theta}}(\boldsymbol{x},\boldsymbol{z})}
     {q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})}
     \bigg] = \mathcal{L}(\boldsymbol{x}) &amp;&amp; \textit{this is the ELBO} 
\end{aligned}
$$

&lt;/p&gt;
&lt;/div&gt;

&lt;p class="body"&gt;

Going forward, we will use &lt;span class = "math"&gt; \(\mathcal{L}(\boldsymbol{x})\) &lt;/span&gt; as our objective function of concern where 

&lt;span class="math"&gt; \(q_{\boldsymbol{\phi}}
(\boldsymbol{z}|\boldsymbol{x})\) &lt;/span&gt; 

is essentially a surrogate distribution for 

&lt;span class="math"&gt; \(p_{\boldsymbol{\theta}}(\boldsymbol{z|x}) \)&lt;/span&gt;. 

Now, 
&lt;span class="math"&gt; \(q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})\) &lt;/span&gt; 

will parameterized be by a deep network where we assume:

&lt;/p&gt;

&lt;ul class="body"&gt;

&lt;li&gt;

&lt;span class="math"&gt; 

\(q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x}) = 
q_{\boldsymbol{\phi}}(\boldsymbol{z}^2) q_{\boldsymbol{\phi}}(\boldsymbol{z}^1|
\boldsymbol{z}^2) q_{\boldsymbol{\phi}}(\boldsymbol{x}|
\boldsymbol{z}^1)\)

&lt;li&gt;

&lt;span class="math"&gt; \(z_i^k \perp z_j^k | \boldsymbol{x} \cup \boldsymbol{z}^{k-1}
\ \forall \ i \neq j,k \) &lt;/span&gt; 

&lt;/li&gt;

&lt;/ul&gt;

&lt;p class="body"&gt;

This is known as &lt;a href="https://www.cs.cmu.edu/~epxing/Class/10708-17/notes-17/10708-scribe-lecture13.pdf" style="color:#FF7D33"&gt;mean field approximation&lt;/a&gt; which 
yields a &lt;b&gt; tractable &lt;/b&gt; surrogate distribution.

&lt;h1 class="subhead"&gt; 3.1.1 Making More Sense of the ELBO &lt;/h1&gt;
&lt;p class="body"&gt;

Understanding some variations of the ELBO really sheds light on the underlining intuitions of Variational Inference. For starters, we can rewrite the ELBO in the following form:

&lt;/p&gt;

&lt;div class="math"&gt;
&lt;p class="formula"&gt;
$$
\begin{aligned}
\mathcal{L}(\boldsymbol{x}) &amp;=  
\mathbb{E}_{\boldsymbol{z}\sim q_{\boldsymbol{\phi}}(\boldsymbol{z|x})}
\big[\log p_{\boldsymbol{\theta}}(\boldsymbol{z|x})
 p_{\boldsymbol{\theta}}(\boldsymbol{x})\big]
- \mathbb{E}_{\boldsymbol{z}\sim q_{\boldsymbol{\phi}}(\boldsymbol{z|x})}
\big[\log q_{\boldsymbol{\phi}}(\boldsymbol{z|x})\big] \\
&amp;=  
\mathbb{E}_{\boldsymbol{z}\sim q_{\boldsymbol{\phi}}(\boldsymbol{z|x})}
\big[\log p_{\boldsymbol{\theta}}(\boldsymbol{z|x})
+ \log p_{\boldsymbol{\theta}}(\boldsymbol{x})\big]
- \mathbb{E}_{\boldsymbol{z}\sim q_{\boldsymbol{\phi}}(\boldsymbol{z|x})}
\big[\log q_{\boldsymbol{\phi}}(\boldsymbol{z|x})\big] \\
&amp;= 
\mathbb{E}_{\boldsymbol{z}\sim q_{\boldsymbol{\phi}}(\boldsymbol{z|x})}
\big[\log p_{\boldsymbol{\theta}}(\boldsymbol{z|x})\big]
+ \log p_{\boldsymbol{\theta}}(\boldsymbol{x})
\mathbb{E}_{\boldsymbol{z}\sim q_{\boldsymbol{\phi}}(\boldsymbol{z|x})}[] 
- \mathbb{E}_{\boldsymbol{z}\sim q_{\boldsymbol{\phi}}(\boldsymbol{z|x})}
\big[\log q_{\boldsymbol{\phi}}(\boldsymbol{z|x})\big] \\
&amp;= 
 \log p_{\boldsymbol{\theta}}(\boldsymbol{x}) - 
 \text{D}_{\text{KL}}(q_{\boldsymbol{\phi}}(\boldsymbol{z|x}) || 
p_{\boldsymbol{\theta}}(\boldsymbol{z|x}) )
\end{aligned}
$$

&lt;/p&gt;
&lt;/div&gt;

&lt;p class="body"&gt; 
Note, the second expectation on the third line does not touch 

&lt;span class="math"&gt;\(p_{\boldsymbol{\theta}}(\boldsymbol{x})\) &lt;/span&gt;,

so that expectation evaluates to 1. Furthermore, we should notice in the last line 
&lt;a href = "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" style="color:#FF7D33"&gt; KL Divergence&lt;/a&gt; 

has made an appearance—which is a non-negative number that quantifies the distance between two distributions; where 0 is returned if the two distributions under concern are identical.
&lt;/p&gt;

&lt;figure&gt;
  &lt;img class="fig" src="/images/vae/kl-divergence.png" width="100%"&gt;
  &lt;figcaption class="figcaption"&gt; 
  Fig 2. Depiction of KL Divergence
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p class = "body"&gt;
So, maximizing the lower bound is actually maximizing our original log probability and subtracting the distance between the intractable distribution and the surrogate distribution. Theoretically, we could set 

&lt;span class = "math"&gt; \(
q_{\boldsymbol{\phi}}(\boldsymbol{z|x})  = q_{\boldsymbol{\theta}}(\boldsymbol{z|x})\)
&lt;/span&gt; 

and our lower bound would be &lt;b&gt;tight&lt;/b&gt;!

&lt;/p&gt;


&lt;script type="text/javascript"&gt;

if (!document.getElementById('mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        fonts: ['STIX', 'TeX']," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}

&lt;/script&gt;&lt;hr&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;p id='vae'&gt;Kingma Diederik and Welling Max.
Auto-encoding variational bayes.
2014.
URL: &lt;a href="https://arxiv.org/pdf/1312.6114.pdf"&gt;https://arxiv.org/pdf/1312.6114.pdf&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-vae-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='wakeSleep'&gt;Geoffrey Hinton, Peter Dayan, Brendan&amp;nbsp;J Frey, and Radford&amp;nbsp;M Neal.
The wake-sleep alogrithm for unsupervised neural networks.
1995.
URL: &lt;a href="http://www.cs.toronto.edu/~fritz/absps/ws.pdf"&gt;http://www.cs.toronto.edu/~fritz/absps/ws.pdf&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-wakeSleep-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Angel C. Hernandez</dc:creator><pubDate>Tue, 01 Jan 2019 00:00:00 -0500</pubDate><guid isPermaLink="false">tag:None,2019-01-01:/test-post.html</guid><category>pgm</category><category>generative_models</category></item></channel></rss>