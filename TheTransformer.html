<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>  The Transformer | angel's blog </title>
  <link rel="canonical" href="/TheTransformer.html">


  <link rel="stylesheet" href="/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="/theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="/theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="/theme/css/theme.css">
  <link rel="stylesheet" href="/theme/css/md-style.css">

  <link rel="alternate" type="application/atom+xml" title="Full Atom Feed" href="/feeds/all.atom.xml">
  <link rel="alternate" type="application/rss+xml" title="Full RSS Feed" href="/feeds/all.rss.xml">  
  <link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">
  <meta name="description" content="TEXT TEXT">
</head>

<body>
    <header class="header">
        <div class="container">
            <div class="row">
                <div class="col-sm-8">
                    <h1 class="title"><a href="/">angel's blog</a></h1>
                </div>
            </div>    
        </div>
    </header>
    <div class="main">
        <div class="container">
            <h1 class="blogTitle">The Transformer</h1>
            <hr>
            <article class="article">
                <header>
                    <ul class="list-inline">
                        <li class="list-inline-item text-muted" title="2019-07-20T00:00:00-04:00">
                            <i class="fa fa-clock-o"></i>
                            ???
                        </li>
                        <li class="list-inline-item">
                            <i class="fa fa-folder-open-o"></i>
                            <a href="/category/probabilistic-graphical-models.html">Deep Learning</a>
                        </li>
                        <li class="list-inline-item">
                            <i class="fa fa-user-o"></i>
                            <a href="/author/angel-c-hernandez.html">Angel C. Hernandez</a>          
                        </li>
                    </ul>
                </header>
                <div class="content">
                    <h1 class="head">1. Introduction </h1>
                    <p class="body">???</p>
                    <br>
                    <h1 class="head">2. Recurrent Neural Network Review </h1>
                    <p class="body">
                        We cannot really talk about attention and the Transformer without mentioning its
                        predecessor, Recurrent Neural Networks (RNNs). RNNs are typically your deep model of choice when you are 
                        performing supervised learning on sequential data, e.g., you have an input sequence
                        <span class="math"> \(\pmb{x} = (\pmb{x}_1, ..., \pmb{x}_T)\) </span> and a corresponding target sequence 
                        of the same length, <span class="math"> \(\pmb{y} = (y_1, ..., y_T)\) </span>. We will assume
                        a given input is a real-valued vector, <span class="math">\(\pmb{x}_t \in \mathbb{R}^D \)</span> and a given 
                        target is a scalar token, <span class="math">\(y_t \in (1, 2,..., K)\)</span>. In this setting, we want to learn
                        a network that will minimize the discriminative negative log likelihood: 
                        <span class="math">\(-\log P(\pmb{y}|\pmb{x}) = -\sum_{t=1}^T \log P(y_t|\pmb{x}_1, ..., \pmb{x}_t)\)</span>.
                        Moving forward, we will briefly review different RNN architectures with hope this is a review to the audience, but 
                        if not follow up links which review RNNs in depth have been provided.  
                    </p>
                    <br>
                    <h1 class="subhead">2.1 Vanilla RNN</h1>
                    <p class="body">
                        In the case of a vanilla rnn the model encodes each input, <span class="math">\( \pmb{x}_t\)</span>, 
                        into a hidden state, <span class="math">\( \pmb{h}_t\)</span>, where the hidden state is a function 
                        of the input and the previous hidden state, i.e., <span class="math">\( \pmb{h}_t = f(\pmb{h}_{t-1}, \pmb{x}_t)\)</span>.
                        The function simply maps <span class="math">\(\pmb{h}_{t-1} \text{ and } \pmb{x}_t\)</span> to a new vector space by applying
                        a linear combination using the learnable matrices, <span class="math">\(\pmb{W} \text{ and } \pmb{U}\)</span>, respectively.
                        These learnable matrices are shared across all time steps in a given layer, where typically a RNN will have multiple 
                        layers stacked on top of each other and each layer would have its own set of learnable matrices. The output at a given time step
                        is linear combination of the last hidden state layer passed into the softmax function which generates the model's prediction,
                        <span class="math"> \(\hat{y}_t\)</span>. Ultimately, each hidden state is sending information to the next hidden state which encodes 
                        information of all inputs up until that point in time. As a result, the model is using encoded information about 
                        <span class="math">\(\pmb{x}_1, \pmb{x}_2, ..., \pmb{x}_t\)</span> to generate the prediction, <span class="math"> \(\hat{y}_t\)</span>.
                        The computational graph of a vanilla RNN can be found in <span class="figtext">Figure 1</class>.
                        <figure>
                            <br>
                            <img class="fig" src="/images/transformer/rnn.png" width="70%", height="30%">
                            <br>
                            <figcaption class="figcaption"> 
                            <span class="figtext">Fig. 1</span> Vanilla single layer RNN.
                            </figcaption>
                            <br>
                        </figure>
                    </p>
                    <br>
                    <h1 class="subhead">2.2 Bidirectional RNN</h1>
                    <p class="body">
                        Bidirectional RNNs are similar to standard RNNs, except in a given layer there are hidden states traveling forwards, 
                        <span class="math"> \(\overrightarrow{\pmb{h}}_t\)</span>, and backwards, <span class="math">\(\overleftarrow{\pmb{h}_t}\)</span>.
                        Each direction has their own set of learnable matrices and a given layer at a given time step effectively encodes information 
                        about the entire input sequence, <span class="math">\(\pmb{x}_1, \pmb{x}_2, ..., \pmb{x}_T\)</span>. The computational graph 
                        of a bidirectional rnn can be found in <span class="figtext">Figure 2</span>.
                        <figure>
                            <br>
                            <img class="fig" src="/images/transformer/bidirectional-rnn.png" width="70%", height="30%">
                            <br>
                            <figcaption class="figcaption"> 
                            <span class="figtext">Fig. 2</span> Single layer bidirectional RNN.
                            </figcaption>
                            <br>
                        </figure>
                        <a href="https://www.deeplearningbook.org/" target="_blank">
                            <button class="button">For more on RNNs see Chapter 10 from Deep Learning book.</button>
                        </a>
                    </p>
                    <br>
                    <h1 class="subhead">2.3 LSTM Network</h1>
                    <p class="body">
                        While standard RNNs can be powerful functions approximators, performance suffers on longer sequences 
                        due to <a href="https://www.youtube.com/watch?v=qhXZsFVxGKo" target="_blank" class="hyperlinks">vanishing/exploding</a> gradients.
                        LSTMs were developed to mitigate this issue and are probably one of the most popular types of RNNs. We begin by defining the following linear combination:
                    </p>

                    <div class="formula">
                        <br>
                        $$
                        \text{Score}: \pmb{a}_t^j= \pmb{W}^j\pmb{h}_{t-1} + 
                        \pmb{U}^j\pmb{x}_{t} + \pmb{b}^j
                        $$
                        <br>
                    </div>

                    <p class="body">
                        <span class="math">\( j \)</span> is simply the "type" of score where each type of score warrants a different set of parameters: 
                        <span class="math">
                            \( \{\pmb{W}^j,\pmb{U}^j,\pmb{b}^j\} \)</span>. We can now define the 
                            formulas used to calculate the state of a given hidden unit, 
                            <span class="math">\( \pmb{h}_t \)</span>, within a LSTM:
                    </p>

                    <div class="formula">
                        <br>
                        $$
                        \begin{aligned}
                        \text{Forget Gate}&: \pmb{f}_t = \sigma(\pmb{a}_t^f) \\
                        \text{Input Gate}&: \pmb{i}_t = 1-\pmb{f}_t \\
                        \text{Output Gate}&: \pmb{\omega}_t = \sigma(\pmb{a}_t^\omega) \\ \\
                        \text{Candidate Values} &: \tilde{\pmb{C}}_t = \text{tanh}({\pmb{a}_t^c}) \\ 
                        \text{Cell State} &: \pmb{C}_t = \pmb{f}_t\odot \pmb{C}_{t-1} + 
                        \pmb{i}_t\odot\tilde{\pmb{C}}_t \\
                        \text{Hidden Unit}&: \pmb{h}_{t} = \pmb{\omega}_t\text{tanh}(\pmb{C}_t)
                        \end{aligned}
                        $$
                        <br>
                    </div>

                    <p class="body">
                        The <b>Cell State</b> encompasses a bulk of the work and is the reason LSTMs can 
                        <i>share</i> and <i>retain</i> information over distant time intervals. In the Cell State we 
                        have the <b>Forget Gate</b> and <b>Input Gate</b> which determine how much
                        information to utilize from the past and present, respectively. After calculating 
                        the Cell State, the network uses an <b>Output Gate</b> to determine how much information to send to the next time step,  
                        <span class="math">\( \pmb{h}_{t+1} \)</span>. All of this is done "internally" 
                        within a given hidden unit and the computational graph is identical to the ones 
                        defined above. 
                        <figure>
                            <br>
                            <img class="fig" src="/images/transformer/LSTM.png" width="70%", height="30%">
                            <br>
                            <figcaption class="figcaption"> 
                            <span class="figtext">Fig. 3</span> LSTM Network extended from
                            <a class="hyperlinks" 
                            href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
                            target="_blank">
                            colah's blog</a>.
                            </figcaption>
                            <br>
                    </figure>
                    <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">
                        <button class="button">For more on LSTMs see colah's blog.</button>
                    </a>
                    </p>
                    <br>
                    <h1 class="subhead">2.4 Sequence to Sequence Model</h1>
                    <p class="body">
                        Now we have the <b>seq2seq</b> model which was introduced by 
                        <a class="papers" 
                        href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf"
                        target="_blank">
                        Sutskever et al. 2014
                        </a> and is a popular choice when performing 
                        <a class="hyperlinks" href="https://google.github.io/seq2seq/nmt/" target="_blank">
                        machine translation</a> tasks. The model feeds an input sequence, say a sentence,
                        into an <b>Encoder</b> network and the network learns to represent the input 
                        as a <i><b>thought vector</i></b>; where this thought vector is suppose encode <b>all</b> the information 
                        about the input sequence. Then, the vector is feed into the <b>Decoder</b> network and this network learns 
                        to predict the output sequence, say the most likely reply to the input sequence. The hidden state in the Decoder,
                        <span class="math"> \(\pmb{s}_t\)</span>, is a function of the previous ground truth target token, 
                        <span class="math">\(y_{t-1}\)</span> (which acts as the input), and the previous decoder hidden state,
                        <span class="math"> \(\pmb{s}_{t-1}\)</span>, i.e., <span class="math">\(\pmb{s}_t = f(y_{t-1}, \pmb{s}_{t-1})\)</span>.
                        At training time you will typically randomly select the ground truth token or the predicted token, 
                        <span class="math">\(\hat{y}_{t-1}\)</span>, to be the input to the decoder where this is known as 
                        <a class="hyperlinks" target="_blank" href="https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/">teacher forcing</a> and was 
                        originally proposed in <a class="papers" target="_blank" href="https://arxiv.org/pdf/1506.03099.pdf">Bengio et al. 2015</a>.
                        The Encoder and Decoder networks are RNNs of your choice, e.g. vanilla, bidirectional or LSTM, and a depiction of a seq2seq
                        model can be found in <span class="figtext">Figure 4</span>.
                        <figure>
                                <br>
                                <img class="fig" src="/images/transformer/seq2seq.png" width="100%">
                                <br>
                                <figcaption class="figcaption"> 
                                <span class="figtext">Fig. 4</span> Seq2seq model with LSTM networks as the encoder and
                                decoder. Note, boxes are LSTM hidden units.
                                </figcaption>
                                <br>
                        </figure>
                    </p>
                    <p class="body">
                        While seq2seq models perform great on <b>short</b> input sequences, 
                        <a class="papers" href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank">
                        Cho et al. 2014</a> proved performance significantly suffers as the size of the
                        input sequence grows (see <span class="figtext">Figure 5</span>). Ultimately, encoding the entire input sequence 
                        into a single thought vector is not adequate to achieve great accuracy on target sequences of longer length. 
                        <figure>
                                <br>
                                <img class="fig" src="/images/transformer/bleu-scores.png" width="100%">
                                <br>
                                <figcaption class="figcaption"> 
                                <span class="figtext">Fig. 5</span> The BLEU scores achieved on different seq2seq models. Image taken
                                from 
                                <a class="papers" href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank">
                                Cho et al. 2014</a>.
                                </figcaption>
                                <br>
                        </figure>
                        <a href="https://docs.chainer.org/en/stable/examples/seq2seq.html" target="_blank">
                            <button class="button">For more on seq2seq models see Chainer.</button>
                        </a>
                    </p>
                    <br>
                    <h1 class="subhead">2.5 Seq2Seq With Attention</h1>
                    <p class="body">
                        Now we get into attention which was first proposed in <a class="papers" target="_blank" href="https://arxiv.org/pdf/1409.0473.pdf">
                        Bahdanau et al. 2015</a> and has been one of the most influential deep learning papers published in the last 10 years. 
                        Paraphrasing the authors, the use of a fixed-length vector, <i>thought vector</i>, is a bottleneck in improving the performance of seq2seq models 
                        and the decoder should be able to (soft-)search parts of the source input that are relevant to predicting a target token at a given time step.
                        Letting <span class="math">\(\pmb{h}_x\)</span> be the set of hidden states in the final layer of the encoder, i.e. 
                        <span class="math">\(\pmb{h}_x = (\pmb{h}_1^2, \pmb{h}_2^2..., \pmb{h}_{T_x}^2)\)</span>, attention proposes to computer a <b>score</b>,
                        <span class="math"> \(e^i_t\)</span>, between a given encoder hidden state and the decoder hidden state at a given time step, i.e.
                        <span class="math">\(e^i_t = f(\pmb{h}_i^2, \pmb{s}_t)\)</span>. We then apply the softmax function over all computed scores
                        and take a weighted average between the softmax-scores, <span class="math">\(\pmb{\alpha}_t\)</span>, and encoder hidden states,
                        <span class="math">\(\pmb{h}_x\)</span>, to obtain a context vector, <span clas="math">\(\pmb{c}_t\)</span>. The context vector 
                        is used to generate a prediction at the time step, <i>t</i>, where the decoder's prediction for a given time step is computed as followed:
                        <div class="formula">
                            <br>
                            $$
                            \begin{aligned}
                            \text{Decoder Hidden State}&: \pmb{s}_t = \text{LSTM}([y_{t-1};\pmb{c}_{t-1}], \pmb{s}_{t-1}) \\
                            \text{MLP Score}&: e_t^i = \pmb{v}^\top\text{tanh}(\pmb{W}_e[\pmb{s}_t;\pmb{h}_i^2]) \\
                            \text{Softmax Score}&: \alpha_t^i = \frac{\exp(e_t^i)}{\sum\limits_{k=1}^{T_x} \exp(e_t^k)}\\
                            \text{Context Vector}&: \pmb{c}_t = \sum\limits_{i=1}^{T_x} \alpha_t^i\pmb{h}_i^2\\
                            \text{Prediction}&: \hat{y}_t = \text{softmax}(\pmb{V}^\top[\pmb{s}_t;\pmb{c}_t] + \pmb{b})
                            \end{aligned}
                            $$
                            <br>
                        </div>
                    </p>
                    <p class="body">
                        When analyzing the softmax-scores at a given time step the model typically learns a sharp distribution where most attention 
                        was allocated to one or two input tokens. This adds a nice layer of interpretability and the softmax-scores typically yields intuitive
                        results (see <span class="figtext">Figure 6</span>). 
                        <figure>
                            <br>
                            <img class="fig" src="/images/transformer/attention.png" width="50%", height="10%">
                            <br>
                            <figcaption class="figcaption"> 
                            <span class="figtext">Fig. 6</span> A machine translation example of attention taken from a
                            <a class="hyperlinks" target="_blank" href="https://www.tensorflow.org/tutorials/text/nmt_with_attention">TensorFlow tutorial</a>.
                            We notice this model paid most attention to the word <i>cold</i> when predicting <i>frio</i> which is the most 
                            intuitive token we'd expect the model to allocate the most attention to.
                            </figcaption>
                            <br>
                    </figure>
                    <a href=https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" target="_blank">
                        <button class="button">For more on attention see Lil'Log.</button>
                    </a>
                    </p>
                    <br>
                    <h1 class="head">3. The Transformer Architecture</h1>
                    <p class="body">
                        We will now review the The Transformer which was originally proposed in <a class="papers" target="_blank" href="https://arxiv.org/pdf/1706.03762.pdf">Vaswani et al. 2017</a> and 
                        was the first of its kind to address traditional natural language processing problems, e.g. language models, machine translation, etc., while only using attention <b>not</b> in 
                        a seq2seq setting. <span class="figtext">Figure 7</span> highlights the network's architecture where in the coming sections we will open up each layer of The Transformer. 
                    <figure>
                        <br>
                        <img class="fig" src="/images/transformer/transformer.png" width="50%" height="10%">
                        <br>
                        <figcaption class="figcaption">
                            <span class="figtext">Fig. 7</span> Transformer architecture taken from <a class="papers" target="_blank" href="https://arxiv.org/pdf/1706.03762.pdf">Vaswani et al. 2017</a>.
                        </figcaption>
                        <br>
                    </figure>
                    </p>
                    <h1 class="subhead">3.1 Embedding and Positional Encoding</h1>
                    <p class="body">
                        Assume we are in a machine translation setting where an input sequence is a set of token words,
                        <span class="math"> \(\pmb{x} = (x_0, ..., x_{T_x-1}) \text{ where } x_t \in (1, ..., K_{x})\)</span>, and the target variable 
                        is also a set of token words of variable length, 
                        <span class="math"> \(\pmb{y} = (y_0, ..., y_{T_y-1}) \text{ where } y_t \in (1, ..., K_{y})\)</span>. Referring to Figure 7, 
                        the network's encoder (left side of the image) maps each input token to its own embedding, 
                        <span class="math"> \(\pmb{\tilde{x}}_t, \text{where } \pmb{\tilde{x}}_t \in \mathbb{R}^{d_{\text{model}}} \text{ and } d_{\text{model}} = 512\)</span>. 
                        Embeddings are a popular concept in neural networks as it allows you to represent a scalar, <span class="math">\(x_t\)</span>, in some high-dimensional space, in this case 
                        512 dimensions. At the beginning of training each token will be assigned a random vector of size 512 and the model learns the best way to represent each 
                        token by back propagating all to the input, <span class="math">\(\pmb{\tilde{x}}_t\)</span>, i.e. during training 
                        <span class="math">\(\pmb{\tilde{x}}_t \leftarrow \pmb{\tilde{x}}_t - \alpha\nabla_{\pmb{\tilde{x}}_t}\mathcal{L}(\pmb{x})\)</span> where <span class="math">\(\alpha\)</span> is the learning rate. 
                    </p>
                    <p class="body">
                        Next, the model calculates a positional encoding for each input token in the sequence and adds them to the corresponding input embedding. 
                        Currently, the model has no notion of <b>word order</b> (1st word, 2nd word), so the model uses a positional encoding, 
                        <span class="math">\(\pmb{pe} \text{ where } \pmb{pe}_t \in \mathbb{R}^{d_{\text{model}}}\)</span>, to inject this information.
                        For a given input example, <span class="math">\(\pmb{x}^{(i)}\)</span>, the model will calculate <span class="math">\(T_{x^{(i)}}\)</span> different 
                        positional encodings where a given positional encoding at a given dimension in the vector, <span class="math"> \(pe_t(n)\)</span> where <i>n</i> is the dimension, is calculated as followed:
                        <div class="formula">
                            <br>
                            $$
                            \begin{aligned}
                            pe_t(2n) &= \text{sin}\bigg(\frac{t}{10000^{\frac{2n}{d_{\text{model}}}}}\bigg) \\
                            pe_t(2n+1) &= \text{cos}\bigg(\frac{t}{10000^{\frac{2n}{d_{\text{model}}}}}\bigg)
                            \end{aligned}
                            $$
                            <br>
                        </div>
                    </p>
                    <p class="body">
                        where <span class="math">\(t \in [0, T_{x^{(i)}}-1]\)</span> and <span class="math">\(n \in [0, \frac{d_{\text{model}}}{2})\)</span>. Ultimately, all 
                        even dimensions of <span class="math">\(\pmb{pe}_t\)</span> would use the sin function and all odd dimensions would you use the cosine function. The model
                        then adds <span class="math">\(\pmb{pe}_t\)</span> with <span class="math">\(\pmb{\tilde{x}}_t\)</span> to obtain a positional embedded vector, denoted as 
                        <span class="math">\(\pmb{\hat{x}}_t\)</span>, where, for example, the <b>3rd</b> input token's positional embedded vector would be calculated as followed:
                        <div class="formula">
                            <br>
                            $$
                            \begin{aligned}
                                \pmb{pe}_2 &= \bigg[\sin\big(\frac{2}{10000^{0}}\big), \cos\big(\frac{2}{10000^{0}}\big), ..., \cos\big(\frac{2}{10000^{\frac{2*255}{512}}}\big)\bigg] \\
                                \pmb{\hat{x}}_2 &= \pmb{\tilde{x}}_2 + \pmb{pe}_2
                            \end{aligned}
                            $$
                            <br>
                        </div>
                    </p>
                    <p class="body">
                        If the reader is curious as to why this encoding injects positional information, I would suggest they review this 
                        <a class="hyperlinks" target="_blank" href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/">post</a>. 
                    </p>
                    <br>
                    <h1 class="subhead">3.2.1 Self-Attention Intuition</h1>
                    <p class="body">
                        The next step in the encoder is multi-head attention but first we will dive into <b>self-attention</b>, as multi-head attention is self-attention replicated many times. The goal of self-attention is
                        for each input vector, <span class="math">\(\pmb{\hat{x}}_t\)</span>, to learn a relationship between <b>itself</b> and the <b>entire sequence of inputs</b>, <span class="math">\((\pmb{\hat{x}}_0, ..., \pmb{\hat{x}}_{T_x-1})\)</span>.
                        Before we dive into the math, let us review <span class="figtext">Figure 8</span> to gain some intuition into what self-attention is really doing. We notice the second word in the sentence, <i>Law</i>, is the subject and it learned to pay most attention to its own
                        descriptors: <i>never be perfect</i> and <i>application</i>. While it isn't a perfect example, <span class="figtext">Figure 8</span> does highlight what self-attention is intended to accomplish. 
                        <figure>
                            <br>
                            <img class="fig" src="/images/transformer/self-attention.png" width="80%" height="40%">
                            <figcaption class="figcaption">
                                <span class="figtext">Fig. 8</span> An example of self-attention taken from <a class="papers" target="_blank" href="https://arxiv.org/pdf/1706.03762.pdf">Vaswani et al. 2017</a>.
                            </figcaption>
                            <br>
                        </figure>
                    </p>
                    <h1 class="subhead">3.2.2 Self-Attention at a Single Time Step</h1>
                    <p class="body">
                        We will now review the computational graph of self-attention for a given example at a given time step. Note, I have not seen a blog/tutorial describe self-attention the way I am about to,
                        but I think it ties nicely with the attention we typically see within a traditional seq2seq setting (section 2.5). The authors of The Transformer paper describe attention using 
                        keys (<span class="math">\(\pmb{k}_{x,t}^l\)</span>), queries (<span class="math">\(\pmb{q}_{x,t}^l\)</span>) and, values (<span class="math">\(\pmb{v}_{x,t}^l\)</span>) where the <i>x</i> 
                        denotes we are in the encoder, <i>t</i> is a given time step and <i>l</i> is a given layer of the encoder (assume <i>l</i> = 1 in the following example). We are going to use these three variables to arrive at a context vector,
                        <span class="math"> \(\pmb{c}_{x,t}^l\)</span>, which encodes a relationship between a given input token, <span class="math">\(x_t\)</span>, and the entire input sequence <span class="math">\(\pmb{x}\)</span>.
                        First, we will obtain the <b>keys, queries</b> and <b>values</b> by applying three separate linear projections on the positional embedded vector, <span class="math">\(\pmb{\hat{x}}_t\)</span>, using three 
                        learnable matrices, <span class="math">\(\pmb{W}_{x,k}^{l}, \pmb{W}_{x,q}^{l} \text{ and } \pmb{W}_{x,v}^{l} \text{ where } \pmb{W}_{x,\{k, q, v\}}^{l} \in \mathbb{R}^{64 \times 512}\).
                        Note, we will need to compute respective key, queries and values for each token in the input sequence in order to arrive at a given context vector.
                        After obtaining all key, queries and values we are in a traditional attention setting where softmax attention weights, <span class="math">\(\pmb{\alpha}_{x,t}^l\)</span> are derived from a score computed between 
                        the <b>keys</b> and <b>queries</b>, i.e. <span class="math">\(\text{score} = f(\pmb{k}_{x,t}^l, \pmb{q}_{x,j}^l)\)</span>. Finally, we will arrive at a given context vector by computing the weighted average between
                        the <b>softmax attention weights</b> and <b>values</b>. <span class="figtext">Figure 9</span> highlights the computational graph used to obtain the context vector at the 1st time step, <span class="math">\(\pmb{c}_{x,0}^1\)</span>.
                        <figure>
                            <br>
                            <img class="fig" src="/images/transformer/self-attention-one-time-step.png" width="80%" height="40%">
                            <figcaption class="figcaption">
                                <span class="figtext">Fig. 9</span> An example of self-attention taken from <a class="papers" target="_blank" href="https://arxiv.org/pdf/1706.03762.pdf">Vaswani et al. 2017</a>.
                            </figcaption>
                            <br>
                        </figure>
                    </p>
                    <div id="disqus_thread"></div>
                    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><hr>
                </div>
            </article>
        </div>
    </div>
    <footer class="footer">
        <div class="container">
            <div class="row">
                <ul class="col-sm-6 list-inline">
                    <li> 
                        Generated by <a href="https://github.com/getpelican/pelican" target="_blank">Pelican</a>
                        / <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>
                    </li>
                </ul>
            </div>    
        </div>
    </footer>
</body>

<!-- mathjax script -->
<script type="text/javascript">
    if (!document.getElementById('mathjaxscript_pelican_#%&#64;#$&#64;#')) {
        var align = "center",
            indent = "0em",
            linebreak = "false";
    
        if (false) {
            align = (screen.width < 768) ? "left" : align;
            indent = (screen.width < 768) ? "0em" : indent;
            linebreak = (screen.width < 768) ? 'true' : linebreak;
        }
    
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%&#64;#$&#64;#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    
        var configscript = document.createElement('script');
        configscript.type = 'text/x-mathjax-config';
        configscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: '"+ align +"'," +
            "    displayIndent: '"+ indent +"'," +
            "    showMathMenu: true," +
            "    messageStyle: 'normal'," +
            "    tex2jax: { " +
            "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        fonts: ['STIX', 'TeX']," +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
            "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
            "    }, " +
            "}); " +
            "if ('default' !== 'default') {" +
                "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                    "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                    "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                    "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                    "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                    "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
                "});" +
                "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                    "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                    "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                    "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                    "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                    "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
                "});" +
            "}";
    
        (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
</script>
<!--- not sure script ??? -->
<script>
        $(document).ready(function(){
          $('[data-toggle="tooltip"]').tooltip(); 
        });
</script>
<!-- disque comments script -->
<script>
    var disqus_config = function () {
    this.page.url = "https://ahernandez105.github.io/TheTransformer.html";
    this.page.identifier ="2";
    };
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://angels-blog-1.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
</html>