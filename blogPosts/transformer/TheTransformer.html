<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>  The Transformer | angel's blog </title>
  <link rel="canonical" href="/TheTransformer.html">


  <link rel="stylesheet" href="/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="/theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="/theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="/theme/css/theme.css">
  <link rel="stylesheet" href="/theme/css/md-style.css">

  <link rel="alternate" type="application/atom+xml" title="Full Atom Feed" href="/feeds/all.atom.xml">
  <link rel="alternate" type="application/rss+xml" title="Full RSS Feed" href="/feeds/all.rss.xml">  
  <link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">
  <meta name="description" content="TEXT TEXT">
</head>

<body>
    <header class="header">
        <div class="container">
            <div class="row">
                <div class="col-sm-8">
                    <h1 class="title"><a href="/">angel__christopher's blog</a></h1>
                </div>
            </div>    
        </div>
    </header>
    <div class="main">
        <div class="container">
            <h1 class="blogTitle">The Transformer</h1>
            <hr>
            <article class="article">
                <header>
                    <ul class="list-inline">
                        <li class="list-inline-item text-muted">
                            <i class="fa fa-clock-o"></i>
                            Last Edited: Sun 28 February 2021
                        </li>
                        <li class="list-inline-item text-muted">
                            <i class="fa fa-user-o"></i> Angel C. Hernandez          
                        </li>
                    </ul>
                </header>
                <div class="content">
                    <!-- SECTION -->
                    <h1 class="head">1. Introduction</h1>
                    <p class="body">
                        The Transformer, <a class="papers" href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Attention Is All You Need</a>, 
                        has taken the deep learning community by storm over the past three years and has not let up. Its applications ranges from 
                        language models like <a class="hyperlinks" href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank">BERT</a> and feature
                        importance selection on tabular data like <a class="hyperlinks" href="https://arxiv.org/pdf/2012.06678.pdf" target="_blank">TabNet</a>.
                        If you strive to stay current on state-of-the-art deep learning models, the Transformer is definitely a network
                        you want to be comfortable with. This post will breakdown each module in the network and hopefully provided 
                        sufficient insight on the network's architecture. 
                    </p>
                    <br>

                    <!-- SECTION -->
                    <h1 class="head">2. Recurrent Neural Network Review </h1>
                    <p class="body">
                        We cannot really talk about attention and the Transformer without mentioning its
                        predecessor, Recurrent Neural Networks (RNNs). RNNs are typically your deep model of choice when you are 
                        performing supervised learning on sequential data, e.g., you have an input sequence
                        <span class="math"> \(\pmb{x} = (\pmb{x}_1, ..., \pmb{x}_T)\) </span> and a corresponding target sequence 
                        of the same length, <span class="math"> \(\pmb{y} = (y_1, ..., y_T)\) </span>. We will assume
                        a given input is a real-valued vector, <span class="math">\(\pmb{x}_t \in \mathbb{R}^D \)</span> and a given 
                        target is a scalar token, <span class="math">\(y_t \in (1, 2,..., K)\)</span>. In this setting, we want to learn
                        a network that will minimize the discriminative negative log likelihood: 
                        <span class="math">\(-\log P(\pmb{y}|\pmb{x}) = -\sum_{t=1}^T \log P(y_t|\pmb{x}_1, ..., \pmb{x}_t)\)</span>.
                        Moving forward, we will briefly review different RNN architectures with hope this is a review to the audience. If not, follow up links which review RNNs in-depth have been provided.  
                    </p>
                    <br>

                    <!-- SECTION -->
                    <h1 class="subhead">2.1 Vanilla RNN</h1>
                    <p class="body">
                        In the case of a vanilla rnn the model encodes each input, <span class="math">\( \pmb{x}_t\)</span>, 
                        into a hidden state, <span class="math">\( \pmb{h}_t\)</span>, where the hidden state is a function 
                        of the input and the previous hidden state, i.e., <span class="math">\( \pmb{h}_t = f(\pmb{h}_{t-1}, \pmb{x}_t)\)</span>.
                        The function simply maps <span class="math">\(\pmb{h}_{t-1} \text{ and } \pmb{x}_t\)</span> to a new vector space by applying
                        a linear combination using the learnable matrices, <span class="math">\(\pmb{W} \text{ and } \pmb{U}\)</span>, respectively.
                        These learnable matrices are shared across all time steps in a given layer, where typically a RNN will have multiple 
                        layers stacked on top of each other and each layer would have its own set of learnable matrices. The output at a given time step
                        is linear combination of the last hidden state layer passed into the softmax function which generates the model's prediction,
                        <span class="math"> \(\hat{y}_t\)</span>. Ultimately, each hidden state is sending information to the next hidden state which encodes 
                        information of all inputs up until that point in time. As a result, the model is using encoded information about 
                        <span class="math">\(\pmb{x}_1, \pmb{x}_2, ..., \pmb{x}_t\)</span> to generate the prediction, <span class="math"> \(\hat{y}_t\)</span>.
                        The computational graph of a vanilla RNN can be found in <span class="figtext">Figure 1</class>.
                        <figure>
                            <br>
                            <img class="fig" src="images/rnn.png" width="70%", height="30%">
                            <br>
                            <figcaption class="figcaption"> 
                            <span class="figtext">Fig. 1</span> Vanilla single layer RNN.
                            </figcaption>
                            <br>
                        </figure>
                    </p>

                    <br>
                    <!-- SECTION -->
                    <h1 class="subhead">2.2 Bidirectional RNN</h1>
                    <p class="body">
                        Bidirectional RNNs are similar to standard RNNs, except in a given layer there are hidden states traveling forwards, 
                        <span class="math"> \(\overrightarrow{\pmb{h}}_t\)</span>, and backwards, <span class="math">\(\overleftarrow{\pmb{h}_t}\)</span>.
                        Each direction has their own set of learnable matrices and a given layer at a given time step effectively encodes information 
                        about the entire input sequence, <span class="math">\(\pmb{x}_1, \pmb{x}_2, ..., \pmb{x}_T\)</span>. The computational graph 
                        of a bidirectional rnn can be found in <span class="figtext">Figure 2</span>.
                        <figure>
                            <br>
                            <img class="fig" src="images/bidirectional-rnn.png" width="80%", height="40%">
                            <br>
                            <figcaption class="figcaption"> 
                            <span class="figtext">Fig. 2</span> Single layer bidirectional RNN.
                            </figcaption>
                            <br>
                        </figure>
                        <a href="https://www.deeplearningbook.org/" target="_blank">
                            <button class="button">For more on RNNs see Chapter 10 from Deep Learning book.</button>
                        </a>
                    </p>

                    <br>
                    <!-- SECTION -->
                    <h1 class="subhead">2.3 LSTM Network</h1>
                    <p class="body">
                        While standard RNNs can be powerful functions approximators, performance suffers on longer sequences 
                        due to <a href="https://www.youtube.com/watch?v=qhXZsFVxGKo" target="_blank" class="hyperlinks">vanishing/exploding</a> gradients.
                        LSTMs were developed to mitigate this issue and are probably one of the most popular types of RNNs. We begin by defining the following linear combination:
                    <div class="formula">
                        <br>
                        $$
                        \text{Score}: \pmb{a}_t^j= \pmb{W}^j\pmb{h}_{t-1} + 
                        \pmb{U}^j\pmb{x}_{t} + \pmb{b}^j
                        $$
                        <br>
                    </div>
                    </p>
                    <p class="body">
                        <span class="math">\( j \)</span> is simply the "type" of score where each type of score warrants a different set of parameters: 
                        <span class="math">
                            \( \{\pmb{W}^j,\pmb{U}^j,\pmb{b}^j\} \)</span>. We can now define the 
                            formulas used to calculate the state of a given hidden unit, 
                            <span class="math">\( \pmb{h}_t \)</span>, within a LSTM:
                    <div class="formula">
                        <br>
                        $$
                        \begin{aligned}
                        \text{Forget Gate}&: \pmb{f}_t = \sigma(\pmb{a}_t^f) \\
                        \text{Input Gate}&: \pmb{i}_t = 1-\pmb{f}_t \\
                        \text{Output Gate}&: \pmb{\omega}_t = \sigma(\pmb{a}_t^\omega) \\ \\
                        \text{Candidate Values} &: \tilde{\pmb{C}}_t = \text{tanh}({\pmb{a}_t^c}) \\ 
                        \text{Cell State} &: \pmb{C}_t = \pmb{f}_t\odot \pmb{C}_{t-1} + 
                        \pmb{i}_t\odot\tilde{\pmb{C}}_t \\
                        \text{Hidden Unit}&: \pmb{h}_{t} = \pmb{\omega}_t\text{tanh}(\pmb{C}_t)
                        \end{aligned}
                        $$
                        <br>
                    </div>
                    </p>
                    <p class="body">
                        The <b>Cell State</b> encompasses a bulk of the work and is the reason LSTMs can 
                        <i>share</i> and <i>retain</i> information over distant time intervals. In the Cell State we 
                        have the <b>Forget Gate</b> and <b>Input Gate</b> which determine how much
                        information to utilize from the past and present, respectively. After calculating 
                        the Cell State, the network uses an <b>Output Gate</b> to determine how much information to send to the next time step,  
                        <span class="math">\( \pmb{h}_{t+1} \)</span>. All of this is done "internally" 
                        within a given hidden unit and the computational graph is identical to the ones 
                        defined above. 
                        <figure>
                            <br>
                            <img class="fig" src="images/LSTM.png" width="70%", height="30%">
                            <br>
                            <figcaption class="figcaption"> 
                            <span class="figtext">Fig. 3</span> LSTM Network extended from
                            <a class="hyperlinks" 
                            href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
                            target="_blank">
                            colah's blog</a>.
                            </figcaption>
                            <br>
                    </figure>
                    <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">
                        <button class="button">For more on LSTMs see colah's blog.</button>
                    </a>
                    </p>

                    <br>
                    <!-- SECTION -->
                    <h1 class="subhead">2.4 Sequence to Sequence Model</h1>
                    <p class="body">
                        Now we have the <b>seq2seq</b> model which was introduced by 
                        <a class="papers" 
                        href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf"
                        target="_blank">
                        Sutskever et al. 2014</a> and is a popular choice when performing 
                        <a class="hyperlinks" href="https://google.github.io/seq2seq/nmt/" target="_blank">
                        machine translation</a> tasks. The model feeds an input sequence, say a sentence,
                        into an <b>Encoder</b> network and the network learns to represent the input 
                        as a <i><b>thought vector</i></b>; where this thought vector is suppose encode <b>all</b> the information 
                        about the input sequence. Then, the vector is feed into the <b>Decoder</b> network and this network learns 
                        to predict the output sequence, say the most likely reply to the input sequence. The hidden state in the Decoder,
                        <span class="math"> \(\pmb{s}_t\)</span>, is a function of the previous ground truth target token, 
                        <span class="math">\(y_{t-1}\)</span> (which acts as the input), and the previous decoder hidden state,
                        <span class="math"> \(\pmb{s}_{t-1}\)</span>, i.e., <span class="math">\(\pmb{s}_t = f(y_{t-1}, \pmb{s}_{t-1})\)</span>.
                        At training time you will typically randomly select the ground truth token or the predicted token, 
                        <span class="math">\(\hat{y}_{t-1}\)</span>, to be the input to the decoder where this is known as 
                        <a class="hyperlinks" target="_blank" href="https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/">teacher forcing</a> and was 
                        originally proposed in <a class="papers" target="_blank" href="https://arxiv.org/pdf/1506.03099.pdf">Bengio et al. 2015</a>.
                        The Encoder and Decoder networks are RNNs of your choice, e.g. vanilla, bidirectional or LSTM, and a depiction of a seq2seq
                        model can be found in <span class="figtext">Figure 4</span>.
                        <figure>
                                <br>
                                <img class="fig" src="images/seq2seq.png" width="100%">
                                <br>
                                <figcaption class="figcaption"> 
                                <span class="figtext">Fig. 4</span> Seq2seq model with LSTM networks as the encoder and
                                decoder. Note, boxes are LSTM hidden units.
                                </figcaption>
                                <br>
                        </figure>
                    </p>
                    <p class="body">
                        While seq2seq models perform great on <b>short</b> input sequences, 
                        <a class="papers" href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank">
                        Cho et al. 2014</a> proved performance significantly suffers as the size of the
                        input sequence grows (see <span class="figtext">Figure 5</span>). Ultimately, encoding the entire input sequence 
                        into a single thought vector is not adequate to achieve great accuracy on target sequences of longer length. 
                        <figure>
                                <br>
                                <img class="fig" src="images/bleu-scores.png" width="100%">
                                <br>
                                <figcaption class="figcaption"> 
                                <span class="figtext">Fig. 5</span> The BLEU scores achieved on different seq2seq models. Image taken
                                from 
                                <a class="papers" href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank">
                                Cho et al. 2014</a>.
                                </figcaption>
                                <br>
                        </figure>
                        <a href="https://docs.chainer.org/en/stable/examples/seq2seq.html" target="_blank">
                            <button class="button">For more on seq2seq models see Chainer.</button>
                        </a>
                    </p>
                    <br>

                    <!-- SECTION -->
                    <h1 class="subhead">2.5 Seq2Seq With Attention</h1>
                    <p class="body">
                        Now we get into attention which was first proposed in <a class="papers" target="_blank" href="https://arxiv.org/pdf/1409.0473.pdf">
                        Bahdanau et al. 2015</a> and has been one of the most influential deep learning papers published in the last 10 years. 
                        Paraphrasing the authors, the use of a fixed-length vector, <i>thought vector</i>, is a bottleneck in improving the performance of seq2seq models 
                        and the decoder should be able to (soft-)search parts of the source input that are relevant to predicting a target token at a given time step.
                        Letting <span class="math">\(\pmb{h}_x\)</span> be the set of hidden states in the final layer of the encoder, i.e. 
                        <span class="math">\(\pmb{h}_x = (\pmb{h}_1^2, \pmb{h}_2^2..., \pmb{h}_{T_x}^2)\)</span>, attention proposes to computer a <b>score</b>,
                        <span class="math"> \(e^i_t\)</span>, between a given encoder hidden state and the decoder hidden state at a given time step, i.e.
                        <span class="math">\(e^i_t = f(\pmb{h}_i^2, \pmb{s}_t)\)</span>. We then apply the softmax function over all computed scores
                        and take a weighted average between the softmax-scores, <span class="math">\(\pmb{\alpha}_t\)</span>, and encoder hidden states,
                        <span class="math">\(\pmb{h}_x\)</span>, to obtain a context vector, <span clas="math">\(\pmb{c}_t\)</span>. The context vector 
                        is used to generate a prediction at the time step, <i>t</i>, where the decoder's prediction for a given time step is computed as followed:
                        <div class="formula">
                            <br>
                            $$
                            \begin{aligned}
                            \text{Decoder Hidden State}&: \pmb{s}_t = \text{LSTM}([y_{t-1};\pmb{c}_{t-1}], \pmb{s}_{t-1}) \\
                            \text{MLP Score}&: e_t^i = \pmb{v}^\top\text{tanh}(\pmb{W}_e[\pmb{s}_t;\pmb{h}_i^2]) \\
                            \text{Softmax Score}&: \alpha_t^i = \frac{\exp(e_t^i)}{\sum\limits_{k=1}^{T_x} \exp(e_t^k)}\\
                            \text{Context Vector}&: \pmb{c}_t = \sum\limits_{i=1}^{T_x} \alpha_t^i\pmb{h}_i^2\\
                            \text{Prediction}&: \hat{y}_t = \text{softmax}(\pmb{V}^\top[\pmb{s}_t;\pmb{c}_t] + \pmb{b})
                            \end{aligned}
                            $$
                            <br>
                        </div>
                    </p>
                    <p class="body">
                        When analyzing the softmax-scores at a given time step the model typically learns a sharp distribution where most attention 
                        was allocated to one or two input tokens. This adds a nice layer of interpretability and the softmax-scores typically yields intuitive
                        results (see <span class="figtext">Figure 6</span>). 
                        <figure>
                            <br>
                            <img class="fig" src="images/attention.png" width="50%", height="10%">
                            <br>
                            <figcaption class="figcaption"> 
                            <span class="figtext">Fig. 6</span> A machine translation example of attention taken from a
                            <a class="hyperlinks" target="_blank" href="https://www.tensorflow.org/tutorials/text/nmt_with_attention">TensorFlow tutorial</a>.
                            We notice this model paid most attention to the word <i>cold</i> when predicting <i>frio</i> which is the most 
                            intuitive token we'd expect the model to allocate the most attention to.
                            </figcaption>
                            <br>
                    </figure>
                    <a href=https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" target="_blank">
                        <button class="button">For more on attention see Lil'Log.</button>
                    </a>
                    </p>

                    <br>
                    <!-- SECTION -->
                    <h1 class="head">3. The Transformer Architecture</h1>
                    <p class="body">
                        We will now review the The Transformer which was originally proposed in <a class="papers" target="_blank" href="https://arxiv.org/pdf/1706.03762.pdf">Vaswani et al. 2017</a> and 
                        was the first of its kind to address traditional natural language processing problems, e.g. language models, machine translation, etc., while only using attention <b>not</b> in 
                        a seq2seq setting. <span class="figtext">Figure 7</span> highlights the network's architecture where in the coming sections we will open up each layer of The Transformer. 
                    <figure>
                        <img class="fig" src="images/transformer.png" width="60%" height="20%">
                        <br>
                        <figcaption class="figcaption">
                            <span class="figtext">Fig. 7</span> Transformer architecture taken from <a class="papers" target="_blank" href="https://arxiv.org/pdf/1706.03762.pdf">Vaswani et al. 2017</a>.
                        </figcaption>
                        <br>
                    </figure>
                    </p>

                    <br>
                    <!-- SECTION -->
                    <h1 class="head">4.0 Embedding and Positional Encoding</h1>
                    <p class="body">
                        Assume we are in a machine translation setting where an input sequence is a set of token words,
                        <span class="math"> \(\pmb{x} = (x_0, ..., x_{T_x-1}) \text{ where } x_t \in (1, ..., K_{x})\)</span>, and the target variable 
                        is also a set of token words of variable length, 
                        <span class="math"> \(\pmb{y} = (y_0, ..., y_{T_y-1}) \text{ where } y_t \in (1, ..., K_{y})\)</span>. Referring to Figure 7, 
                        the network's encoder (left side of the image) maps each input token to its own embedding, 
                        <span class="math"> \(\pmb{\tilde{x}}_t, \text{where } \pmb{\tilde{x}}_t \in \mathbb{R}^{d_{\text{model}}} \text{ and } d_{\text{model}} = 512\)</span>. 
                        Embeddings are a popular concept in neural networks as it allows you to represent a scalar, <span class="math">\(x_t\)</span>, in some high-dimensional space, in this case 
                        512 dimensions. At the beginning of training each token will be assigned a random vector of size 512 and the model learns the best way to represent each 
                        token by back propagating all to the input, <span class="math">\(\pmb{\tilde{x}}_t\)</span>, i.e. during training 
                        <span class="math">\(\pmb{\tilde{x}}_t \leftarrow \pmb{\tilde{x}}_t - \alpha\nabla_{\pmb{\tilde{x}}_t}\mathcal{L}(\pmb{x})\)</span> where <span class="math">\(\alpha\)</span> is the learning rate. 
                    </p>
                    <p class="body">
                        Next, the model calculates a positional encoding for each input token in the sequence and adds them to the corresponding input embedding. 
                        Currently, the model has no notion of <b>word order</b> (1st word, 2nd word), so the model uses a positional encoding, 
                        <span class="math">\(\pmb{pe} \text{ where } \pmb{pe}_t \in \mathbb{R}^{d_{\text{model}}}\)</span>, to inject this information.
                        For a given input example, <span class="math">\(\pmb{x}^{(i)}\)</span>, the model will calculate <span class="math">\(T_{x^{(i)}}\)</span> different 
                        positional encodings where a given positional encoding at a given dimension in the vector, <span class="math"> \(pe_t(n)\)</span> where <i>n</i> is the dimension, is calculated as followed:
                        <div class="formula">
                            <br>
                            $$
                            \begin{aligned}
                            pe_t(2n) &= \text{sin}\bigg(\frac{t}{10000^{\frac{2n}{d_{\text{model}}}}}\bigg) \\
                            pe_t(2n+1) &= \text{cos}\bigg(\frac{t}{10000^{\frac{2n}{d_{\text{model}}}}}\bigg)
                            \end{aligned}
                            $$
                            <br>
                        </div>
                    </p>
                    <p class="body">
                        where <span class="math">\(t \in [0, T_{x^{(i)}}-1]\)</span> and <span class="math">\(n \in [0, \frac{d_{\text{model}}}{2})\)</span>. Ultimately, all 
                        even dimensions of <span class="math">\(\pmb{pe}_t\)</span> would use the sin function and all odd dimensions would you use the cosine function. The model
                        then adds <span class="math">\(\pmb{pe}_t\)</span> with <span class="math">\(\pmb{\tilde{x}}_t\)</span> to obtain a positional embedded vector, denoted as 
                        <span class="math">\(\pmb{\hat{x}}_t\)</span>, where, for example, the <b>3rd</b> input token's positional embedded vector would be calculated as followed:
                        <div class="formula">
                            <br>
                            $$
                            \begin{aligned}
                                \pmb{pe}_2 &= \bigg[\sin\big(\frac{2}{10000^{0}}\big), \cos\big(\frac{2}{10000^{0}}\big), ..., \cos\big(\frac{2}{10000^{\frac{2*255}{512}}}\big)\bigg] \\
                                \pmb{\hat{x}}_2 &= \pmb{\tilde{x}}_2 + \pmb{pe}_2
                            \end{aligned}
                            $$
                            <br>
                        </div>
                    </p>
                    <p class="body">
                        If the reader is curious as to why this encoding injects positional information, I would suggest they review this 
                        <a class="hyperlinks" target="_blank" href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/">post</a>. 
                    </p>

                    <br>
                    <!-- SECTION -->
                    <h1 class="head">5.0 Self-Attention Intuition</h1>
                    <p class="body">
                        The next step in the encoder is multi-head attention but first we will dive into <b>self-attention</b>, as multi-head attention is self-attention replicated many times. The goal of self-attention is
                        for each input vector, <span class="math">\(\pmb{\hat{x}}_t\)</span>, to learn a relationship between <b>itself</b> and the <b>entire sequence of inputs</b>, <span class="math">\((\pmb{\hat{x}}_0, ..., \pmb{\hat{x}}_{T_x-1})\)</span>.
                        Before we dive into the math, let us review <span class="figtext">Figure 8</span> to gain some intuition into what self-attention is really doing. We notice the second word in the sentence, <i>Law</i>, is the subject and it learned to pay most attention to its own
                        descriptors: <i>be, perfect</i> and <i>application</i>. While it isn't a perfect example, <span class="figtext">Figure 8</span> does highlight what self-attention is intended to accomplish. 
                        <figure>
                            <br>
                            <img class="fig" src="images/self-attention.png" width="80%" height="40%">
                            <figcaption class="figcaption">
                                <span class="figtext">Fig. 8</span> An example of self-attention taken from <a class="papers" target="_blank" href="https://arxiv.org/pdf/1706.03762.pdf">Vaswani et al. 2017</a>.
                            </figcaption>
                            <br>
                        </figure>
                    </p>

                    <br>
                    <!-- SECTION -->
                    <h1 class="subhead">5.1 Self-Attention at a Single Time Step</h1>
                    <p class="body">
                        We will now review the computational graph of self-attention for a given example at a given time step. Note, I have not seen a blog/tutorial describe self-attention the way I am about to,
                        but I think it ties nicely with the attention we typically see within a traditional seq2seq setting (section 2.5). The authors of The Transformer paper describe attention using 
                        keys (<span class="math">\(\pmb{k}_{x,t}^{l,h}\)</span>), queries (<span class="math">\(\pmb{q}_{x,t}^{l,h}\)</span>) and, values (<span class="math">\(\pmb{v}_{x,t}^{l,h}\)</span>) where the <i>x</i> 
                        denotes we are in the encoder, <i>t</i> is a given time step, <i>l</i> is a given layer of the encoder  and <i>h</i> is a given head (assume <i>l</i> and <i>h</i> = 1 in the following example). We are going to use these three variables to arrive at a context vector,
                        <span class="math"> \(\pmb{c}_{x,t}^{l,h}\)</span>, which encodes a relationship between a given input token, <span class="math">\(x_t\)</span>, and the entire input sequence <span class="math">\(\pmb{x}\)</span>.
                        First, we will obtain the <b>keys, queries</b> and <b>values</b> by applying three separate linear projections on the positional embedded vector, <span class="math">\(\pmb{\hat{x}}_t\)</span>, using three 
                        learnable matrices, <span class="math">\(\pmb{W}_{x,k}^{l,h}, \pmb{W}_{x,q}^{l,h} \text{ and } \pmb{W}_{x,v}^{l,h} \text{ where } \pmb{W}_{x,\{k, q, v\}}^{l,h} \in \mathbb{R}^{512 \times 64}\).
                        Note, we will need to compute respective key, queries and values for each token in the input sequence in order to arrive at a given context vector.
                        After obtaining all key, queries and values we are in a traditional attention setting where softmax attention weights, <span class="math">\(\pmb{\alpha}_{x,t}^{l,h}\)</span>, are derived from a score computed between 
                        a given <b>key</b> and <b>query</b>, i.e. <span class="math">\(\text{score} = f(\pmb{k}_{x,t}^l, \pmb{q}_{x,j}^l)\)</span>. Finally, we will arrive at a given context vector by computing the weighted average between
                        the <b>softmax attention weights</b> and <b>values</b>. <span class="figtext">Figure 9</span> highlights the computational graph used to obtain the context vector at the 1st time step in the 1st layer for the 1st head <span class="math">\(\pmb{c}_{x,0}^{1,1}\)</span>.
                        <figure>
                            <br>
                            <img class="fig" src="images/self-attention-one-time-step.png" width="80%" height="40%">
                            <figcaption class="figcaption">
                                <span class="figtext">Fig. 9</span> Computational graph of self-attention at the 1st time step in the 1st layer for the 1st head of the Transformer encoder. Note, the subscript, <i>x</i>, which was 
                                used in section 5.1 to denote encoder variables was omitted for brevity. 
                            </figcaption>
                            <br>
                        </figure>
                    </p>
                    <p class="body">
                        We notice the score function, <span class="math">\(e_{x,(t,j)}^{l,h}\)</span>, is simply a dot product between the query of concern,
                        in this example <span class="math">\(\pmb{q}_{x,0}^{1,1}\)</span>, and a given key, <span class="math">\(\pmb{k}_{x,j}^{1,1}\)</span>, normalized 
                        by the square root of the projection dimensionality, 64. Hopefully this example highlights self-attention is <b>identical</b> to traditional attention in seq2seq models, 
                        where rather than computing a relationship between a given decoder hidden state and set of encoder hidden states, <b>we are computing a relationship between 
                        a given input token over the set of all input tokens, using query, key and value projections</b>.
                    </p>
                    <br>

                    <!-- SECTION -->
                    <h1 class="subhead">5.2 Self-Attention over an Entire Sequence and Multi-Head Attention</h1>
                    <p class="body">
                        One major advantage of the Transformer as opposed to seq2seq models is it computationally faster because it can compute context vectors for an entire 
                        sequence in one go. Let us define 
                        <span class="math">\(\pmb{\hat{X}} = [\pmb{\hat{x}_0}^\top; ...; \pmb{\hat{x}_{T_x-1}}^\top]\)</span> which is just concatenating the sequence of positional 
                        embeddings as row vectors where <span class="math">\(\pmb{\hat{X}} \in \mathbb{R}^{T_x\times 512}\)</span>. We can obtain the key, query and value matrices 
                        by applying the linear transformation matrices to <span class="math">\(\pmb{\hat{X}}\)</span>, e.g., <span class="math">\(\pmb{Q}_x^{1,1} = \pmb{\hat{X}}\pmb{W}_{x,q}^{1,1}\)</span>.
                        Having the key, query and value matrices readily available the set of context vectors over the entire input sequence, <span class="math">\(\pmb{C}_x^{l,h}\)</span>, can be calculated
                        as followed:
                        <div class="formula">
                            <br>
                            $$
                            \pmb{C}_x^{l,h} = \text{softmax}\bigg(\frac{\pmb{Q}_x^{l,h}{\pmb{K}_x^{l,h}}^\top}{\sqrt{64}}\bigg)\pmb{V}_x^{l,h}
                            $$
                            <br>
                        </div>
                    </p>
                    <p class="body">
                        Furthermore, all operations used to calculate the context matrix are depicted in <span class="figtext">Figure 10</span>.
                        <figure>
                            <br>
                            <img class="fig" src="images/self-attention-entire-sequence.png" width="80%" height="40%">
                            <figcaption class="figcaption">
                                <span class="figtext">Fig. 10</span> All matrix operations used to obtain self-attention over an entire input sequence in the 1st layer for the 1st head of the encoder. Note, the subscript, <i>x</i>, which was 
                                used in sections 5.2 and 5.1 to denote encoder variables was omitted for brevity. 
                            </figcaption>
                            <br>
                        </figure>
                    </p>
                    <p class="body">
                        Multi-head attention is simply self-attention computed <i>n</i> number of <b>heads</b> times (where <i>n</i> = 8 in the original paper). We simply concatenate each context vector head, 
                        <span class="math">\(\pmb{C}_x^{l,h}\)</span>, as column vectors and arrive at a matrix that contains all attention heads, i.e., 
                        <span class="math">\(\pmb{C}_x^{l} = \big[\pmb{C}_x^{l,1}; ...; \pmb{C}_x^{l,8}\big]\)</span> where <span class="math">\(\pmb{C}_x^{l} \in \mathbb{R}^{T_x\times8\cdot 64}\)</span>.
                        Finally, the model multiplies <span class="math">\(\pmb{C}_x^{l}\)</span> by a linear transformation matrix, <span class="math">\(\pmb{W}_{x,\text{out}}^{l} \in \mathbb{R}^{512\times 512}\)</span>.
                    </p>
                    <br>

                    <!-- SECTION -->
                    <h1 class="head">6.0 Residual Connection and Layer Normalization</h1>
                    <p class="body">
                        We first begin with a residual connection which is a staple connection in <b>deep</b> neural networks, and became most popular from ResNet, 
                        <a class="papers" href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank"> He et al. 2015</a>. We simply add the output of multi-head attention
                        with the inputted positional encoding matrix. Empirically, this has proven
                        to ensure gradients flow nicely through the entire network as more and more layers are added.
                    </p>
                    <p class="body">
                        Next, we perform layer normalization which was originally proposed in <a class="papers" href="https://arxiv.org/pdf/1607.06450.pdf" target="_blank">Ba et al. 2016</a>. This significantly 
                        decreases training time as each hidden layer is to mapped 0 mean 1 standard deviation. Furthermore, it has become more popular than 
                        <a class="hyperlinks" target="_blank" href="https://en.wikipedia.org/wiki/Batch_normalization#:~:text=Batch%20normalization%20(also%20known%20as,and%20Christian%20Szegedy%20in%202015.">Batch Normalization</a> as statistics are calculated over 
                        a given layer as opposed to a mini-batch. The <b>Add & Norm</b> block in the encoder at at the first layer,
                        <span class="math">\(\pmb{LN}_x^1\)</span>, is calculated as followed:
                        <div class="formula">
                            <br>
                            $$
                            \pmb{LN}_x^l = \text{LayerNorm}(\pmb{C}_x^l\pmb{W}_{x,\text{out}}^l + \pmb{\hat{X}})
                            $$
                            <br>
                        </div>
                    </p>

                    <br>
                    <!-- SECTION -->
                    <h1 class="head">7.0 Output of Encoder Layer</h1>
                    <p class="body">
                        In order to arrive at the output of a given encoder layer, <span math="class">\(\pmb{O}_x^l\)</span>, we perform a Feed Forward, residual connection and layer normalization (in that order).
                        The Feed Forward network is defined as followed:

                        <div class="formula">
                            <br>
                            $$
                            \text{FF}_x^l = \text{FeedForward}(\pmb{z}) = \text{relu}(\pmb{z}\pmb{W}_{x,1}^l + \pmb{b}_{x,1}^l)\pmb{W}_{x,2}^l + \pmb{b}_{x,2}^l
                            $$
                            <br>
                        </div>
                    </p>
                    <p class="body">
                        where <span class="math">\(\pmb{W}_{x,1}^l\in \mathbb{R}^{512\times 2048}\)</span> and <span class="math">\(\pmb{W}_{x,2}^l\in \mathbb{R}^{2048\times 512}\)</span>. Finally, the output 
                        of a given encoder layer is calculated as followed:
                        <div class="formula">
                            <br>
                            $$
                            \pmb{O}_x^l = \text{LayerNorm}\big(\text{FeedForward}(\pmb{LN}_{x}^l) + \pmb{LN}_{x}^l\big)
                            $$
                            <br>
                        </div>
                    </p>
                    <p class="body">
                        It should be noted that the output of the <i>lth</i> layer is the input to <i>l</i> + 1 layer as highlighted in <span class="figtext">Figure 11</span>.
                    <figure>
                        <br>
                        <img class="fig" src="images/enc-two-layers.png" width="50%" height="10%">
                        <figcaption class="figcaption">
                            <span class="figtext">Fig. 11</span> Stacked Transformer encoder layers taken from 
                            <a class="hyperlinks" target="_blank" href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>.
                        </figcaption>
                        <br>
                    </figure>
                    </p>

                    <br>
                    <!-- SECTION -->
                    <h1 class="head">8.0 Formulaic Encoder Summary</h1>
                    <p class="body"> 
                        Below summarizes all calculations within a given layer of the encoder.
                        <div class="formula">
                            <br>
                            \begin{alignat*}{3}
                            \text{Input} &:= \pmb{\hat{X}} &&= \text{Embedding}(\pmb{x}) + \text{PE}(\pmb{x}) &&& {\scriptstyle \text{    where  } \pmb{\hat{X}} \ \in \ \mathbb{R}^{T_x \ \times \ 512}}\\
                            \text{Key} &:= \pmb{K}_x^{l,h} &&= \pmb{\hat{X}}\pmb{W}_{x,k}^{l,h} &&& {\scriptstyle \text{    where  } \pmb{W}_{x,k}^{l,h} \ \in \ \mathbb{R}^{512\ \times \ 64}}\\
                            \text{Query} &:= \pmb{Q}_x^{l,h} &&= \pmb{\hat{X}}\pmb{W}_{x,q}^{l,h} &&& {\scriptstyle \text{    where  } \pmb{W}_{x,q}^{l,h} \ \in \ \mathbb{R}^{512\ \times \ 64}}\\
                            \text{Value} &:= \pmb{V}_x^{l,h} &&= \pmb{\hat{X}}\pmb{W}_{x,v}^{l,h} &&& {\scriptstyle \text{    where  } \pmb{W}_{x,v}^{l,h} \ \in \ \mathbb{R}^{512\ \times \ 64}}\\
                            \text{Attention} &:=  \pmb{C}_x^{l,h} &&= \text{softmax}\bigg(\frac{\pmb{Q}_x^{l,h}{\pmb{K}_x^{l,h}}^\top}{\sqrt{64}}\bigg)\pmb{V}_x^{l,h} \\
                            \text{Multi-Head Attention} &:= \pmb{C}_x^l &&= [\pmb{C}_x^{l,1};...;\pmb{C}_x^{l,8}] &&& {\scriptstyle \text{    where  } \pmb{C}_x^l \ \in \ \mathbb{R}^{T_x \ \times \ 512}}\\
                            \text{Add & Norm} &:= \pmb{LN}_x^l &&= \text{LayerNorm}(\pmb{C}_x^l\pmb{W}_{x,\text{out}}^l + \pmb{\hat{X}}) &&& {\scriptstyle \text{    where  } \pmb{W}_{x,\text{out}}^l \ \in \ \mathbb{R}^{512 \ \times \ 512}}\\
                            \text{Feed Forward} &:= \pmb{FF}_x^l &&= \text{FeedForward}(\pmb{LN}_x^l) \\
                            \text{Output} &:= \pmb{O}_x^l &&= \text{LayerNorm}(\pmb{FF}_x^l + \pmb{LN}_x^l) &&& {\scriptstyle \text{    where  } \pmb{O}_{x}^l \ \in \ \mathbb{R}^{T_x \ \times \ 512}}
                            \end{alignat*}
                            <br>
                        </div>
                    </p>
                    <p class="body">
                        Lastly, for any layer greater than 1 you would omit the the Input calculation and replace 
                        <span class="math">\(\pmb{\hat{X}}\)</span> with <span class="math">\(\pmb{O}_x^{l-1}\)</span>.
                    </p>

                    <br>
                    <!-- SECTION -->
                    <h1 class="head">9.0 Decoder Formulaic Summary</h1>
                    <p class="body">
                        The goal of the decoder is to generated a predicted target sequence using information from the encoder and information from predictions made up until the current point in time.
                        The decoder is similar to seq2seq decoders in the sense that it is <b>auto-regressive</b>, consuming previously
                        generated symbols as additional input when generating the next prediction. The Transformer reuses all modules from the 
                        encoder in the decoder using separate learnable matrices, subscript <i>y</i>. Below is the formulaic summary of the decoder.
                        <div class="formula">
                            <br>
                            \begin{alignat*}{3}
                            \text{Input} &:= \pmb{\hat{Y}} &&= \text{Embedding}(\pmb{y}) + \text{PE}(\pmb{y}) &&& {\scriptstyle \text{where  } \pmb{\hat{y}} \ \in \ \mathbb{R}^{T_y \ \times \ 512}}\\
                            \text{Key} &:= \pmb{K}_{y,t}^{l,h} &&= \pmb{\hat{Y}}\pmb{W}_{y,k}^{l,h} &&& {\scriptstyle \text{where  } \pmb{W}_{y,k}^{l,h} \ \in \ \mathbb{R}^{512\ \times \ 64}}\\
                            \text{Query} &:= \pmb{Q}_{y,t}^{l,h} &&= \pmb{\hat{Y}}\pmb{W}_{y,q}^{l,h} &&& {\scriptstyle \text{where  } \pmb{W}_{y,q}^{l,h} \ \in \ \mathbb{R}^{512\ \times \ 64}}\\
                            \text{Value} &:= \pmb{V}_{y,t}^{l,h} &&= \pmb{\hat{Y}}\pmb{W}_{y,v}^{l,h} &&& {\scriptstyle \text{where  } \pmb{W}_{y,v}^{l,h} \ \in \ \mathbb{R}^{512\ \times \ 64}}\\
                            \text{Masked Attention} &:=  \pmb{C}_{y,t}^{l,h} &&= \text{softmax}\bigg(\frac{\pmb{Q}_{y,t}^{l,h}{\pmb{K}_{y,t}^{l,h}}^\top}{\sqrt{64}}\bigg)\pmb{V}_{y,t}^{l,h} \otimes \pmb{mask}_t\\
                            \text{Masked MH Attention} &:= \pmb{C}_{y,t}^l &&= [\pmb{C}_{y,t}^{l,1};...;\pmb{C}_{y,t}^{l,8}] &&& {\scriptstyle \text{where  } \pmb{C}_{y,t}^l \ \in \ \mathbb{R}^{T_y \ \times \ 512}}\\
                            \text{Add & Norm} &:= \pmb{LN}_{y,t}^{\text{one},l} &&= \text{LayerNorm}(\pmb{C}_{y,t}^l\pmb{W}_{y,\text{out}}^l + \pmb{\hat{Y}}) &&& {\scriptstyle \text{where  } \pmb{W}_{y,\text{out}}^l \ \in \ \mathbb{R}^{512 \ \times \ 512}}\\
                            \text{EncDec MH Attention} &:= \pmb{\tilde{C}}_{y,t}^{l} &&= \text{softmax}\bigg(\frac{\pmb{LN}_{y,t}^{\text{one},l}{\pmb{O}_{x}^{l}}^\top}{\sqrt{512}}\bigg)\pmb{O}_{x}^{l} &&& {\scriptstyle \text{where } \pmb{O}_x^l \ \in \ \mathbb{R}^{T_x \ \times \ 512}}\\
                            \text{Add & Norm} &:= \pmb{LN}_{y,t}^{\text{two},l} &&= \text{LayerNorm}(\pmb{\tilde{C}}_{y,t}^l + \pmb{LN}_{y,t}^{\text{one},l}) \\
                            \text{Feed Forward} &:= \pmb{FF}_{y,t}^l &&= \text{FeedForward}(\pmb{LN}_{y,t}^{\text{two},l}) \\
                            \text{Output} &:= \pmb{O}_{y,t}^l &&= \text{LayerNorm}(\pmb{FF}_{y,t}^l + \pmb{LN}_{y,t}^{\text{two},l}) &&& {\scriptstyle \text{where  } \pmb{O}_{y,t}^l \ \in \ \mathbb{R}^{T_y \ \times \ 512}} \\
                            \text{Prediction} &:= \pmb{P}_{y,t} &&= \text{softmax}(\pmb{O}_{y,t}^L\pmb{W}_{\text{pred}}) &&& {\scriptstyle \text{where } \pmb{W}_{\text{pred}} \ \in \mathbb{R}^{512 \ \times \ \text{Vocab}}}
                            \end{alignat*}
                            <br>
                        </div>
                    </p>
                    <p class="body">
                        You'll notice we added a subscript of <i>t</i> which indicates the given timestep we are at within the decoder. Assume the ground truth tokens are
                        <code>&#60bos&#62 this notation is a bit much &#60eos&#62</code>. Next, assume we are at timestep 
                        3, so we have already generated predictions for the tokens <code>&#60bos&#62 this notation</code>. We will calculate the Input, Key, Query and Value matrices and arrive 
                        at <b>Masked Attention</b>. We need to apply a mask to our attention matrix, so the decoder only attends to tokens in previous timesteps. Knowing
                        <span class="math">\(\pmb{mask}_t \in \mathbb{R}^{8\times 512}\)</span>, rows 0-2 would be set to <b>1</b> and rows 3-7 would be set to <b>0</b>. The <b>EncDec Multi-Head Attention</b> module is where 
                        we apply attention on the input sequence, <span class="math">\(\pmb{x}\)</span>. Finally, the prediction at a given timestep is generated by taking the softmax of 
                        a linear mapping of the last layer in of the decoder, <i>L</i>.
                    </p>
                    
                    <br>
                    <!-- SECTION -->
                    <h1 class="head">10.0 Conclusion</h1>
                    <p class="body">
                        That right there is the Transformer in all its formulaic entirety. While the notation can be overwhelming, knowing how each matrix/tensor 
                        is calculated was necessary for me to fully understand this architecture. If you are curious about implementing the network, from scratch, 
                        in Pytorch, then this <a class="hyperlinks" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank">post</a> by Harvard's NLP group is place to go. Cheers!!
                    </p>

                    <br>
                    <!-- SECTION -->
                    <h1 class="head">References</h1>
                    <p class="body">
                        [1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. 30, 2017.<br>
                        [2] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. 27, 2014. <br>
                        [3] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks, 2015. <br>
                        [4] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. 2014.<br>
                        [5] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. pages 770778, 2016.<br>
                        [6] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. 2016<br>
                        [7] Ian J. Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, Cambridge, MA, USA, 2016. http://www.deeplearningbook.org<br>
                        [8] Understanding LSTM Networks colah's blog <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a><br>
                        [9] The Illustrated Transformer Jay Alammar Visualizing machine learning one concept at a time <a href="http://jalammar.github.io/illustrated-transformer/" target="_blank">http://jalammar.github.io/illustrated-transformer/</a><br>
                        [10] Attention? Attention! Lil'Log <a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" target="_blank">https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html</a>
                    </p>
                    <br><br>
                    <div id="disqus_thread"></div>
                    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><hr>
                </div>
            </article>
        </div>
    </div>
    <footer class="footer">
        <div class="container">
            <div class="row">
                <ul class="col-sm-6 list-inline">
                    <li> 
                        Generated by <a href="https://github.com/getpelican/pelican" target="_blank">Pelican</a>
                        / <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>
                    </li>
                </ul>
            </div>    
        </div>
    </footer>
</body>

<!-- mathjax script -->
<script type="text/javascript">
    if (!document.getElementById('mathjaxscript_pelican_#%&#64;#$&#64;#')) {
        var align = "center",
            indent = "0em",
            linebreak = "false";
    
        if (false) {
            align = (screen.width < 768) ? "left" : align;
            indent = (screen.width < 768) ? "0em" : indent;
            linebreak = (screen.width < 768) ? 'true' : linebreak;
        }
    
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%&#64;#$&#64;#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    
        var configscript = document.createElement('script');
        configscript.type = 'text/x-mathjax-config';
        configscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: '"+ align +"'," +
            "    displayIndent: '"+ indent +"'," +
            "    showMathMenu: true," +
            "    messageStyle: 'normal'," +
            "    tex2jax: { " +
            "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        fonts: ['STIX', 'TeX']," +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
            "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
            "    }, " +
            "}); " +
            "if ('default' !== 'default') {" +
                "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                    "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                    "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                    "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                    "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                    "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
                "});" +
                "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                    "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                    "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                    "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                    "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                    "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
                "});" +
            "}";
    
        (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
</script>
<!--- not sure script ??? -->
<script>
        $(document).ready(function(){
          $('[data-toggle="tooltip"]').tooltip(); 
        });
</script>
<!-- disque comments script -->
<script>
    var disqus_config = function () {
    this.page.url = "https://ahernandez105.github.io/TheTransformer.html";
    this.page.identifier ="2";
    };
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://angels-blog-1.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
</html>