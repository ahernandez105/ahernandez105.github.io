<!doctype html>
<html lang="en">
<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <title>  Pixel CNN | angel's blog </title>
  <link rel="canonical" href="/TheTransformer.html">
  <link rel="stylesheet" href="/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="/theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="/theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="/theme/css/theme.css">
  <link rel="stylesheet" href="/theme/css/md-style.css">
  <link rel="alternate" type="application/atom+xml" title="Full Atom Feed" href="/feeds/all.atom.xml">
  <link rel="alternate" type="application/rss+xml" title="Full RSS Feed" href="/feeds/all.rss.xml">  
  <link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">
  <meta name="description" content="TEXT TEXT">
</head>
<body>
    <header class="header">
        <div class="container">
            <div class="row">
                <div class="col-sm-8">
                    <h1 class="title"><a href="/">angel's blog</a></h1>
                </div>
            </div>    
        </div>
    </header>
    <div class="main">
        <div class="container">
            <h1 class="blogTitle">Pixel CNN</h1>
            <hr>
            <article class="article">
                <header>
                    <ul class="list-inline">
                        <li class="list-inline-item text-muted" title="2019-07-20T00:00:00-04:00"><i class="fa fa-clock-o"></i> ???</li>
                        <li class="list-inline-item text-muted"><i class="fa fa-user-o"></i> Angel C. Hernandez</li>
                    </ul>
                </header>
                <div class="content">
                    <!-- SECTION -->
                    <h1 class="head">1. Introduction </h1>
                    <p class="body">
                        Pixel CNN was proposed in <a class="papers" href="https://arxiv.org/pdf/1601.06759.pdf" target="_blank">Oord et al. 2016</a>
                        and is an auto-regressive generative model. It is effectively an <b>auto encoder</b> that honors the auto-regressive property 
                        using masked convolutions. The data consists of a set of images, 
                        <span class="math">\(\mathcal{D} = \{\pmb{x}^{(t)}\}_{t=1}^T \text{ where } \pmb{x}_i \in \mathbb{R}^{c\times n\times n}\)</span>
                        and <i>T</i> = number of examples, <i>c</i> = channels, and <i>n</i> = height = width of an image.
                        The goal is to learn the joint distribution over all pixels using the chain rule of probability:
                        <div class="formula">
                            <br>
                            $$
                            p(\pmb{x}) = \prod_{i=1}^{n^2} p(x_i|x_1,...,x_{i-1})
                            $$
                            <br>
                        </div>
                    </p>
                    <p class="body">
                        The value <span class="math">\(p(x_i|x_1,...,x_{i-1})\)</span> is the likelihood of the <i>ith</i>
                        pixel, <span class="math">\(x_i\)</span>, given the previous pixels 
                        <span class="math">\(x_1,..., x_{i-1}\)</span>. Pixels are conditioned in a row-by-row pixel-by-pixel 
                        fashion which is highlighted in <span class="figtext">Figure 1.</span>
                        <figure>
                            <br>
                            <img class="fig" src="images/context.png" width="40%" height="20%">
                            <figcaption class="figcaption">
                                <span class="figtext">Fig. 1</span> To generate pixel <span class="math">\(x_i\)</span> one 
                                conditions on all previously generated pixels left and above pixel <span class="math">\(x_i\)</span>.
                                Image taken from <a class="papers" href="https://arxiv.org/pdf/1601.06759.pdf" target_="_blank">Oord et al. 2016</a>.
                            </figcaption>
                            <br>
                        </figure>
                    </p>
                    <p class="body">
                        Moving forward, we will step through how to train a Pixel CNN on both black and white images 
                        and color images using Pytorch. All code reviewed in this post can be found at this GitHub 
                        <a class="hyperlinks" href="https://github.com/ahernandez105/pixelCnn" target_="blank">repo</a>.
                        It should be noted that this blog post was inspired after I completed homework 1 of Berkeley's
                        <a class="hyperlinks" href="https://sites.google.com/view/berkeley-cs294-158-sp20/home">Deep Unsupervised Learning Course.</a>. 
                        While my PixelCNN implementation is not identical to their solution, I do borrow their datasets and some helper methods.
                    </p>
                    <br>
                    <!-- SECTION -->
                    <h1 class="head">2. Loading MNIST Dataset</h1>
                    <p class="body">
                        In the repo you will find a pickled dictionary <code>mnist.pkl</code>. The keys
                        <code>'train'</code> and <code>'test'</code> will map you to the train and test numpy array of images, respectively. 
                        Each image is of shape (28, 28, 1) and takes on value between [0, 255]. To simplify the problem, we will make the images
                        binary by assigning pixels values > 127.5 value <b>1</b> and all other pixels value <b>0</b>. We then will create a Pytorch
                        Dataset and DataLoader where a given batch will be of shape (128, 1, 28, 28) and contains <code>'x'</code> and <code>'y'</code> tensors.
                        The <code>'x'</code> tensor is a batch of images normalized to 0 mean 1 std and the <code>'y'</code> tensor is a batch of 
                        ground truth binary images.
                    </p>
<pre style="background-color:#f8f9fa; font-size: 12px">
<code>
import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pickle

class Data(Dataset):
    def __init__(self, array, device='cpu', mean=None, std=None):
        self.N, self.C, self.H, self.W  = array.shape
        self.array = array
        self.device = device
        if mean is None and std is None:
            self.mean = np.mean(self.array, axis=(0,2,3))
            self.std = np.std(self.array, axis=(0,2,3))
        else:
            self.mean = mean
            self.std = std

    def __len__(self):
        return self.N
    
    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.item()
        
        return {
            'x': torch.tensor((self.array[idx] - self.mean)/self.std, dtype=torch.float).to(self.device),
            'y': torch.tensor(self.array[idx], dtype=torch.long).to(self.device)}
    
    @staticmethod
    def collate_fn(batch):
        bsize = len(batch)

        return {
            'x': torch.stack([batch[i]['x'] for i in range(bsize)], dim=0),
            'y': torch.stack([batch[i]['y'] for i in range(bsize)], dim=0)}
    
    @staticmethod
    def read_pickle(fn, dset):
        assert dset=='train' or dset=='test'
        with open(fn, 'rb') as file:
            data = pickle.load(file)[dset]
            data = (data > 127.5).astype('uint8')
            N, H, W, C = data.shape

            return data.reshape(N, C, H, W)

train_arr, test_arr = Data.read_pickle('mnist.pkl', 'train'), Data.read_pickle('mnist.pkl', 'test')
train = DataLoader(Data(train_arr), batch_size=128, shuffle=True, collate_fn=Data.collate_fn)
test = DataLoader(Data(test_arr, mean=train.dataset.mean, std=train.dataset.std), batch_size=128, shuffle=True, collate_fn=Data.collate_fn)

for batch in train:
    print(type(batch))
    print(batch['x'].shape)
    print(batch['y'].shape)
    break
</code>
</pre>

<p class="body">Output:</p>
<div class="console">
    <pre style="white-space: pre-line; color: #ffffff">
    &ltclass 'dict'&gt
    torch.Size([128, 1, 28, 28])
    torch.Size([128, 1, 28, 28])
    </pre>
</div>
<br>
                    <br>
                    <!-- SECTION -->
                    <h1 class="head">3.0 Pixel CNN Architecture Binary Images</h1>
                    <p class="body">
                        Now we will begin to open up the Pixel CNN architecture. We will review each component in the 
                        architecture which is highlighted in the below figures. Note, the below architecture is almost identical 
                        to the one in the original paper. The only differences are we use a Conv 7x7 in the residual block (as opposed to 3x3),
                        we use 64 convolution filters and include normalization layers.
                    </p>
                    <br>
                    <div class="rowFigure">
                        <div class="columnFigure">
                            <figure>
                                <img class="fig" src="images/pixel-cnn.png" width="80%" height="80%">
                                <figcaption class="figcaption">
                                    <span class="figtext">Fig. 2</span> Pixel CNN architecture<br>used in 
                                    <a class="hyperlinks" href="https://sites.google.com/view/berkeley-cs294-158-sp20/home">Berkeley Deepul</a>. 
                                </figcaption>
                            </figure>
                        </div>
                        <div class="columnFigure">
                            <figure>
                                <img class="fig" src="images/residual-pixel-cnn.png" width="85%" height="85%">
                                <figcaption class="figcaption">
                                    <span class="figtext">Fig. 3</span> Pixel CNN residual block <br>used in 
                                    <a class="hyperlinks" href="https://sites.google.com/view/berkeley-cs294-158-sp20/home">Berkeley Deepul</a>. 
                                </figcaption>
                            </figure>
                        </div>
                    </div>

                    <br>
                    <!-- SECTION -->
                    <h1 class="subhead">3.1 Masked Convolution</h1>
                    <p class="body">
                        As a convolution kernel traverses through the input image, <span class="math">\(\pmb{x}\)</span>, 
                        we need to apply a <b>mask</b> to the kernel to ensure pixels <b>to the left</b> and <b>above</b>
                        the current pixel are only used in the convolution operation. This is referred to as the <b style="color:blue">context</b>
                        and is highlighted in <span class="figtext">Figure 1</span>. Below we will show how to generate
                        the different types of masks, A and B.
                    </p>

                    <br>
                    <!-- SECTION -->
                    <h1 class="subhead">3.1.1 Mask Type A Single Channel</h1>
                    <p class="body">
                        One neat thing about this architecture is the width and height of the input image will be maintained 
                        across all layers within the network. For simplicity, let's assume an input image of shape 5x5 with one channel and kernel
                        size of 3x3. Next, use the below formula to determine how much we need to pad our input image to maintain the same width/height:
                        <div class="formula">
                            <br>
                            \begin{aligned}
                            W_{\text{out}} &= \frac{W_{\text{in}} \ - \text{ kernel_size } + \ 2P}{\text{stride}} + 1\\
                            5 &= \frac{5 - 3 + 2P}{1} + 1 \\
                            P &= 1
                            \end{aligned}
                            <br>
                        </div>
                    </p>
                    <p class="body">
                        Next, we need to apply a <b>mask</b> to our kernel to honor the auto-regressive property. <span class="figtext">Figure 4</span> shows 
                        the masked convolution operation for this toy example.
                        <figure>
                            <br>
                            <img class="fig" src="images/mask-type-A.png" width="90%" height="70%">
                            <figcaption class="figcaption">
                                <span class="figtext">Fig. 4</span> Masked type A convolution at two different points in time.
                            </figcaption>
                            <br>
                        </figure>
                    </p>
                    <p class="body">
                        Hopefully it is clear that each stride of the convolution is auto-encoding the current pixel, <span class="math">\(x_i\)</span>.
                        We mask our kernel because all pixels to the right and below the current pixel essentially do not exist 
                        at inference time. Finally, the reason this is <b>Mask A</b> is because 
                        <span class="math">\(h_i = f(\pmb{x}_{< i})\)</span>, i.e. a given pixel in the hidden state is a function of the only the context.
                    </p>

                    <br>
                    <!-- SECTION -->
                    <h1 class="subhead">3.1.2 Mask B Single Channel</h1>
                    <p class="body">
                        In the case of <b>Mask B</b>, <span class="math">\(h_i = f(x_i, \pmb{x}_{< i})\)</span>, i.e. a given pixel in 
                        the hidden state is a function of both the context and the current pixel being auto-encoded. As a result,
                        below is mask type B for this toy example:
                        <div class="formula">
                            <br>
                            $$
                            \begin{bmatrix} 
                                1 & 1 & 1 \\
                                1 & 1 & 0 \\
                                0 & 0 & 0 \\
                            \end{bmatrix}
                            $$
                            <br>
                        </div>
                    </p>

                    <br>
                    <!-- SECTION -->
                    <h1 class="subhead">3.1.3 Masked Convolution Pytorch Single Channel</h1>
                    <p class="body">
                        It is actually pretty simple to implement masked convolution into Pytorch, see below:
                    </p>
<pre style="background-color:#f8f9fa; font-size: 12px">
<code>
import torch
from torch.nn import Conv2d

class MaskedConv2dBinary(Conv2d):
    def __init__(self, m_type, in_channels, out_channels, kernel_size, padding):
        assert m_type=='A' or m_type=='B'
        super().__init__(
            in_channels=in_channels, out_channels=out_channels, 
            kernel_size=kernel_size, stride=1, padding=padding)
        self.register_buffer('mask', torch.zeros_like(self.weight)) # ensures mask is stored in state_dict

        if m_type=='A':
            self.mask[:, :, 0:kernel_size//2, :] = 1
            self.mask[:, :, kernel_size//2, 0:kernel_size//2] = 1
        else:
            self.mask[:, :, 0:kernel_size//2, :] = 1
            self.mask[:, :, kernel_size//2, 0:kernel_size//2 + 1] = 1
    
    def forward(self, x):
        self.weight.data *= self.mask
        return super().forward(x)

conv_a = MaskedConv2dBinary('A', 1, 64, 7, 3)
print('Mask A')
print(conv_a.mask[0,:,:,:])

conv_b = MaskedConv2dBinary('B', 64, 64, 7, 3)
print('Mask B')
print(conv_b.mask[0,0,:,:])
</code>
</pre>

<p class="body">Output:</p>
<div class="console">
    <pre style="white-space: pre-line; color: #ffffff">
        Mask A
        tensor([[[1., 1., 1., 1., 1., 1., 1.],
                    [1., 1., 1., 1., 1., 1., 1.],
                    [1., 1., 1., 1., 1., 1., 1.],
                    [1., 1., 1., 0., 0., 0., 0.],
                    [0., 0., 0., 0., 0., 0., 0.],
                    [0., 0., 0., 0., 0., 0., 0.],
                    [0., 0., 0., 0., 0., 0., 0.]]])
        Mask B
        tensor([[1., 1., 1., 1., 1., 1., 1.],
                [1., 1., 1., 1., 1., 1., 1.],
                [1., 1., 1., 1., 1., 1., 1.],
                [1., 1., 1., 1., 0., 0., 0.],
                [0., 0., 0., 0., 0., 0., 0.],
                [0., 0., 0., 0., 0., 0., 0.],
                [0., 0., 0., 0., 0., 0., 0.]])
    </pre>
</div>
                
                 <br>
                 <!-- SECTION -->
                 <h1 class="subhead">3.2 Layer Normalization</h1>
                 <p class="body">
                    Layer normalization is popular in most deep networks to speed up training by transforming hidden layers to be
                    centered around 0 mean and 1 std. Now, after a given masked convolution operation you will have an outputted 
                    tensor with shape (bsize, filters, height, width) = (128, 64, 28, 28). When I first attempted to incorporate layer normalization
                    I figured I should normalize each filter, i.e. calculate the statistics for each 28x28 matrix. <b>Do not do this!</b> Reason being,
                    this violates the auto-regressive property because you are taking the mean and std over all pixels in a filter. Remember, 
                    a given pixel, <span class="math">\(h^l_i\)</span> should not know anything about 
                    <span class="math">\(h^l_{i+1}, h^l_{i+2}, ..., h^l_{n^2}\)</span>. As result, we must normalize over each pixel position 
                    across all 64 filters. This results in <span class="math">\(28^2\)</span> layer norm operations.
                </p>

<pre style="background-color:#f8f9fa; font-size: 12px">
<code>
import torch
from torch.nn import LayerNorm

class LayerNormPixel(LayerNorm):
    def __init__(self, n_filters, affine=True):
        super().__init__([n_filters], elementwise_affine=affine)
    
    def forward(self, x):
        # permute operation returns tensor of shape
        # (bsize, height, width, filters)
        x = super().forward(x.permute(0, 2, 3, 1).contiguous())

        # return tensor is of shape (bsize, filters, height, width)
        return x.permute(0, 3, 1, 2).contiguous()
</code>
</pre>
                <br>
                <!-- SECTION -->
                <h1 class="subhead">3.3 Residual Block</h1>
                <p class="body">
                    Now that we our layer norm and masked convolution classes implmented, we can create the residual block class.
                </p>

<pre style="background-color:#f8f9fa; font-size: 12px">
<code>
import torch
from torch.nn import ReLU, ModuleList, Module

class ResBlock(Module):
    def __init__(self, m_type, in_channels, out_channels, kernel_size, masked_conv_class):
        super().__init__()
        self.net = ModuleList()
        self.net.append(masked_conv_class(m_type, in_channels, out_channels, 1, 0))
        self.net.append(ReLU())
        p = int((kernel_size - 1)/2)
        self.net.append(masked_conv_class(m_type, in_channels, out_channels, kernel_size, p))
        self.net.append(ReLU())
        self.net.append(masked_conv_class(m_type, in_channels, out_channels, 1, 0))
        self.net.append(LayerNormPixel(out_channels))
        self.net.append(ReLU())
    
    def forward(self, x):
        initial_x = x
        for module in self.net:
            x = module(x)
        
        return initial_x + x
</code>
</pre>
                <br>
                <!-- SECTION -->
                <h1 class="subhead">3.4 Pixel CNN Pytorch</h1>
                <p class="body">
                    Using all created modules above, we can create the Pixel CNN class.
                </p>

<pre style="background-color:#f8f9fa; font-size: 12px">
<code>
class PixelCnn(Module):
    def __init__(
        self, in_dim, channels, kernel_size, layers, filters, 
        dist_size, masked_conv_class):
        super().__init__()
        self.in_dim = in_dim
        self.channels = channels
        self.kernel_size = kernel_size
        self.filters = filters
        self.layers = layers
        self.dist_size = dist_size
        self.mconv = masked_conv_class
        p = int((self.kernel_size - 1)/2)

        self.net = ModuleList()
        self.net.append(self.mconv('A', self.channels, self.filters, self.kernel_size, p))
        self.net.append(LayerNormPixel(self.filters))
        self.net.append(ReLU())
        for _ in range(self.layers-1):
            self.net.append(ResBlock(
                'B', self.filters, self.filters, self.kernel_size, self.mconv))
        self.net.append(self.mconv('B', self.filters, self.filters, 1, 0))
        self.net.append(ReLU())
        self.net.append(self.mconv('B', self.filters, self.dist_size*self.channels, 1, 0))
        
        self.log_softmax = LogSoftmax(dim=1)
        self.loss = NLLLoss(reduction='mean')

    def forward(self, x):
        bsize, _, _, _ = x.shape
        for module in self.net:
            x = module(x)
        
        return self.log_softmax(x).view(bsize, self.dist_size, self.channels, self.in_dim, self.in_dim)

    def get_loss(self, x, y):
        bsize, _, _, _, _ = x.shape
        x1 = x.view((bsize, self.dist_size, self.in_dim*self.in_dim*self.channels))
        y1 = y.view((bsize, self.in_dim*self.in_dim*self.channels))

        return self.loss(x1, y1)

    def generate_samples(self, n, dev, mean, std):
        self.eval()
        samples_in = torch.zeros((n, self.channels, self.in_dim, self.in_dim), dtype=torch.float, device=dev)
        samples_out = torch.zeros((n, self.channels, self.in_dim, self.in_dim), dtype=torch.float, device=dev)

        for row in range(self.in_dim):
            for col in range(self.in_dim):
                for channel in range(self.channels):
                    dist = Categorical(torch.exp(self(samples_in)[:, :, channel, row, col]))
                    s = dist.sample().type(torch.float)
                    samples_in[:, channel, row, col] = (s-mean[channel])/std[channel]
                    samples_out[:, channel, row, col] = s
        
        return samples_out

# two layers so printing area is smaller
model = PixelCnn(28, 1, 7, 2, 64, 2, MaskedConv2dBinary)
print(model)
</code>
</pre>

<p class="body">Output:</p>
<div class="console">
<pre style="color: #ffffff">
PixelCnn(
    (net): ModuleList(
        (0): MaskedConv2dBinary(1, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
        (1): LayerNormPixel(torch.Size([64]), eps=1e-05, elementwise_affine=True)
        (2): ReLU()
        (3): ResBlock(
        (net): ModuleList(
            (0): MaskedConv2dBinary(64, 64, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): MaskedConv2dBinary(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
            (3): ReLU()
            (4): MaskedConv2dBinary(64, 64, kernel_size=(1, 1), stride=(1, 1))
            (5): LayerNormPixel(torch.Size([64]), eps=1e-05, elementwise_affine=True)
            (6): ReLU()
            )
        )
        (4): MaskedConv2dBinary(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (5): ReLU()
        (6): MaskedConv2dBinary(64, 2, kernel_size=(1, 1), stride=(1, 1))
    )
    (log_softmax): LogSoftmax()
    (loss): NLLLoss()
)
</pre>
</div>
                <br><br>
                <div id="disqus_thread"></div>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><hr>
                </div>
            </article>
        </div>
    </div> 
</body>
<!-- mathjax script -->
<script type="text/javascript">
    if (!document.getElementById('mathjaxscript_pelican_#%&#64;#$&#64;#')) {
        var align = "center",
            indent = "0em",
            linebreak = "false";
    
        if (false) {
            align = (screen.width < 768) ? "left" : align;
            indent = (screen.width < 768) ? "0em" : indent;
            linebreak = (screen.width < 768) ? 'true' : linebreak;
        }
    
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%&#64;#$&#64;#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    
        var configscript = document.createElement('script');
        configscript.type = 'text/x-mathjax-config';
        configscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: '"+ align +"'," +
            "    displayIndent: '"+ indent +"'," +
            "    showMathMenu: true," +
            "    messageStyle: 'normal'," +
            "    tex2jax: { " +
            "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        fonts: ['STIX', 'TeX']," +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
            "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
            "    }, " +
            "}); " +
            "if ('default' !== 'default') {" +
                "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                    "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                    "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                    "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                    "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                    "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
                "});" +
                "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                    "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                    "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                    "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                    "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                    "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
                "});" +
            "}";
    
        (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
</script>
<!--- not sure script ??? -->
<script>
        $(document).ready(function(){
          $('[data-toggle="tooltip"]').tooltip(); 
        });
</script>
<!-- disque comments script -->
<script>
    var disqus_config = function () {
    this.page.url = "https://ahernandez105.github.io/TheTransformer.html";
    this.page.identifier ="2";
    };
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://angels-blog-1.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>

<script type="text/javascript">
    window.onload = function(){
        var codeElement = document.getElementById('python_code');
        // Add code mirror class for coloring (default is the theme)
        codeElement.classList.add( 'cm-s-default' );
        var code = codeElement.innerText;

        codeElement.innerHTML = "";

        CodeMirror.runMode(
          code,
          'python',
          codeElement
        );
    };
</script>
</html>