<!doctype html>
<html lang="en">
<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <title>  Pixel CNN | angel's blog </title>
  <link rel="canonical" href="/TheTransformer.html">
  <link rel="stylesheet" href="/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="/theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="/theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="/theme/css/theme.css">
  <link rel="stylesheet" href="/theme/css/md-style.css">
  <link rel="alternate" type="application/atom+xml" title="Full Atom Feed" href="/feeds/all.atom.xml">
  <link rel="alternate" type="application/rss+xml" title="Full RSS Feed" href="/feeds/all.rss.xml">  
  <link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">
  <meta name="description" content="TEXT TEXT">
</head>
<body>
    <header class="header">
        <div class="container">
            <div class="row">
                <div class="col-sm-8">
                    <h1 class="title"><a href="/">angel's blog</a></h1>
                </div>
            </div>    
        </div>
    </header>
    <div class="main">
        <div class="container">
            <h1 class="blogTitle">Pixel CNN</h1>
            <hr>
            <article class="article">
                <header>
                    <ul class="list-inline">
                        <li class="list-inline-item text-muted" title="2019-07-20T00:00:00-04:00"><i class="fa fa-clock-o"></i> ???</li>
                        <li class="list-inline-item text-muted"><i class="fa fa-user-o"></i> Angel C. Hernandez</li>
                    </ul>
                </header>
                <div class="content">
                    <!-- SECTION -->
                    <h1 class="head">1. Introduction </h1>
                    <p class="body">
                        Pixel CNN was proposed in <a class="papers" href="https://arxiv.org/pdf/1601.06759.pdf" target="_blank">Oord et al. 2016</a>
                        and is an auto-regressive generative model. It is effectively an <b>auto encoder</b> that honors the auto-regressive property 
                        using masked convolutions. The data consists of a set of images, 
                        <span class="math">\(\mathcal{D} = \{\pmb{x}^{(t)}\}_{t=1}^T \text{ where } \pmb{x}_i \in \mathbb{R}^{c\times n\times n}\)</span>
                        and <i>T</i> = number of examples, <i>c</i> = channels, and <i>n</i> = height = width of an image.
                        The goal is to learn the joint distribution over all pixels using the chain rule of probability:
                        <div class="formula">
                            <br>
                            $$
                            p(\pmb{x}) = \prod_{i=1}^{n^2} p(x_i|x_1,...,x_{i-1})
                            $$
                            <br>
                        </div>
                    </p>
                    <p class="body">
                        The value <span class="math">\(p(x_i|x_1,...,x_{i-1})\)</span> is the likelihood of the <i>ith</i>
                        pixel, <span class="math">\(x_i\)</span>, given the previous pixels 
                        <span class="math">\(x_1,..., x_{i-1}\)</span>. Pixels are conditioned in a row-by-row pixel-by-pixel 
                        fashion which is highlighted in <span class="figtext">Figure 1.</span>
                        <figure>
                            <br>
                            <img class="fig" src="images/context.png" width="40%" height="20%">
                            <figcaption class="figcaption">
                                <span class="figtext">Fig. 1</span> To generate pixel <span class="math">\(x_i\)</span> one 
                                conditions on all previously generated pixels left and above pixel <span class="math">\(x_i\)</span>.
                                Image taken from <a class="papers" href="https://arxiv.org/pdf/1601.06759.pdf" target="_blank">Oord et al. 2016</a>.
                            </figcaption>
                            <br>
                        </figure>
                    </p>
                    <p class="body">
                        Moving forward, we will step through how to train a Pixel CNN on both black and white images 
                        and color images using Pytorch. All code reviewed in this post can be found at this GitHub 
                        <a class="hyperlinks" href="https://github.com/ahernandez105/pixelCnn" target="_blank">repo</a>.
                        It should be noted that this blog post was inspired after I completed homework 1 of Berkeley's
                        <a class="hyperlinks" href="https://sites.google.com/view/berkeley-cs294-158-sp20/home">Deep Unsupervised Learning Course.</a>. 
                        While my Pixel CNN implementation is not identical to their solution, I do borrow their datasets and some helper methods.
                    </p>
                    <br>
                    <!-- SECTION -->
                    <h1 class="head">2. Loading MNIST Dataset</h1>
                    <p class="body">
                        In the repo you will find a pickled dictionary <code>data/mnist.pkl</code>. The keys
                        <code>'train'</code> and <code>'test'</code> will map you to the train and test numpy array of images, respectively. 
                        Each image is of shape (28, 28, 1) and takes on value between [0, 255]. To simplify the problem, we will make the images
                        binary by assigning pixels values > 127.5 value <b>1</b> and all other pixels value <b>0</b>. We then will create a Pytorch
                        Dataset and DataLoader where a given batch will be of shape (128, 1, 28, 28) and contains <code>'x'</code> and <code>'y'</code> tensors.
                        The <code>'x'</code> tensor is a batch of images normalized to 0 mean 1 std and the <code>'y'</code> tensor is a batch of 
                        ground truth binary images.
                    </p>
<pre style="background-color:#f8f9fa; font-size: 11px">
<code>
import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pickle

class Data(Dataset):
    def __init__(self, array, device='cpu', mean=None, std=None):
        self.N, self.C, self.H, self.W  = array.shape
        self.array = array
        self.device = device
        if mean is None and std is None:
            self.mean = np.mean(self.array, axis=(0,2,3))
            self.std = np.std(self.array, axis=(0,2,3))
        else:
            self.mean = mean
            self.std = std

    def __len__(self):
        return self.N
    
    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.item()
        
        x = self.array[idx].copy().astype(float)
        for i in range(self.C):
            x[i] = (x[i] - self.mean[i])/self.std[i]

        return {
            'x': torch.tensor(x, dtype=torch.float).to(self.device),
            'y': torch.tensor(self.array[idx], dtype=torch.long).to(self.device)}
    
    @staticmethod
    def collate_fn(batch):
        bsize = len(batch)

        return {
            'x': torch.stack([batch[i]['x'] for i in range(bsize)], dim=0),
            'y': torch.stack([batch[i]['y'] for i in range(bsize)], dim=0)}
    
    @staticmethod
    def read_pickle(fn, dset):
        assert dset=='train' or dset=='test'
        with open(fn, 'rb') as file:
            data = pickle.load(file)[dset]
            if 'mnist.pkl' in fn:
                data = (data > 127.5).astype('uint8')
            N, H, W, C = data.shape

            return data.reshape(N, C, H, W)

train_arr, test_arr = Data.read_pickle('mnist.pkl', 'train'), Data.read_pickle('mnist.pkl', 'test')
train = DataLoader(Data(train_arr), batch_size=128, shuffle=True, collate_fn=Data.collate_fn)
test = DataLoader(Data(test_arr, mean=train.dataset.mean, std=train.dataset.std), batch_size=128, shuffle=True, collate_fn=Data.collate_fn)

for batch in train:
    print(type(batch))
    print(batch['x'].shape)
    print(batch['y'].shape)
    break
</code>
</pre>

<p class="body">Output:</p>
<div class="console">
    <pre style="white-space: pre-line; color: #ffffff">
    &ltclass 'dict'&gt
    torch.Size([128, 1, 28, 28])
    torch.Size([128, 1, 28, 28])
    </pre>
</div>
<br>
                    <br>
                    <!-- SECTION -->
                    <h1 class="head">3.0 Pixel CNN Binary Images</h1>
                    <p class="body">
                        Now we will begin to open up the Pixel CNN architecture. We will review each component in the 
                        architecture which is highlighted in the below figures. Note, the below architecture is almost identical 
                        to the one in the original paper. The only differences are we use a Conv 7x7 in the residual block (as opposed to 3x3),
                        we use 64 convolution filters and include normalization layers.
                    </p>
                    <br>
                    <div class="rowFigure">
                        <div class="columnFigure">
                            <figure>
                                <img class="fig" src="images/pixel-cnn.png" width="70%" height="70%">
                                <figcaption class="figcaption">
                                    <span class="figtext">Fig. 2</span> Pixel CNN architecture<br>used in 
                                    <a class="hyperlinks" href="https://sites.google.com/view/berkeley-cs294-158-sp20/home">Berkeley Deepul</a>. 
                                </figcaption>
                            </figure>
                        </div>
                        <div class="columnFigure">
                            <figure>
                                <img class="fig" src="images/residual-pixel-cnn.png" width="75%" height="75%">
                                <figcaption class="figcaption">
                                    <span class="figtext">Fig. 3</span> Pixel CNN residual block <br>used in 
                                    <a class="hyperlinks" href="https://sites.google.com/view/berkeley-cs294-158-sp20/home">Berkeley Deepul</a>. 
                                </figcaption>
                            </figure>
                        </div>
                    </div>
                    <br>

                    <br>
                    <!-- SECTION -->
                    <h1 class="subhead">3.1 Masked Convolution</h1>
                    <p class="body">
                        As a convolution kernel traverses through the input image, <span class="math">\(\pmb{x}\)</span>, 
                        we need to apply a <b>mask</b> to the kernel to ensure pixels <b>to the left</b> and <b>above</b>
                        the current pixel are only used in the convolution operation. This is referred to as the <b style="color:blue">context</b>
                        and is highlighted in <span class="figtext">Figure 1</span>. Below we will show how to generate
                        the different types of masks, A and B.
                    </p>

                    <br>
                    <!-- SECTION -->
                    <h1 class="subhead">3.1.1 Mask Type A Single Channel</h1>
                    <p class="body">
                        One neat thing about this architecture is the width and height of the input image will be maintained 
                        across all layers within the network. For simplicity, let's assume an input image of shape 5x5 with one channel and kernel
                        size of 3x3. Next, use the below formula to determine how much we need to pad our input image to maintain the same width/height:
                        <div class="formula">
                            <br>
                            \begin{aligned}
                            W_{\text{out}} &= \frac{W_{\text{in}} \ - \text{ kernel_size } + \ 2P}{\text{stride}} + 1\\
                            5 &= \frac{5 - 3 + 2P}{1} + 1 \\
                            P &= 1
                            \end{aligned}
                            <br>
                        </div>
                    </p>
                    <p class="body">
                        Next, we need to apply a <b>mask</b> to our kernel to honor the auto-regressive property. <span class="figtext">Figure 4</span> shows 
                        the masked convolution operation for this toy example.
                        <figure>
                            <br>
                            <img class="fig" src="images/mask-type-A.png" width="90%" height="70%">
                            <figcaption class="figcaption">
                                <span class="figtext">Fig. 4</span> Masked type A convolution at two different points in time.
                            </figcaption>
                            <br>
                        </figure>
                    </p>
                    <p class="body">
                        Hopefully it is clear that each stride of the convolution is auto-encoding the current pixel, <span class="math">\(x_i\)</span>.
                        We mask our kernel because all pixels to the right and below the current pixel essentially do not exist 
                        at inference time. Finally, the reason this is <b>Mask A</b> is because 
                        <span class="math">\(h_i^1 = f(\pmb{x}_{< i})\)</span>, i.e. a given pixel in the hidden state is a function of the only the context.
                    </p>

                    <br>
                    <!-- SECTION -->
                    <h1 class="subhead">3.1.2 Mask B Single Channel</h1>
                    <p class="body">
                        In the case of <b>Mask B</b>, <span class="math">\(h_i^l = f(h_i^{l-1}, \pmb{h}_{< i}^{l-1})\)</span>, i.e. a given pixel in 
                        the hidden state is a function of both the context and the current pixel being auto-encoded. As a result,
                        below is mask type B for this toy example:
                        <div class="formula">
                            <br>
                            $$
                            \begin{bmatrix} 
                                1 & 1 & 1 \\
                                1 & 1 & 0 \\
                                0 & 0 & 0 \\
                            \end{bmatrix}
                            $$
                            <br>
                        </div>
                    </p>

                    <br>
                    <!-- SECTION -->
                    <h1 class="subhead">3.1.3 Masked Convolution Pytorch Single Channel</h1>
                    <p class="body">
                        It is actually pretty simple to implement masked convolution into Pytorch, see below:
                    </p>
<pre style="background-color:#f8f9fa; font-size: 11px">
<code>
import torch
from torch.nn import Conv2d

class MaskedConv2dBinary(Conv2d):
    def __init__(self, m_type, in_channels, out_channels, kernel_size, padding):
        assert m_type=='A' or m_type=='B'
        super().__init__(
            in_channels=in_channels, out_channels=out_channels, 
            kernel_size=kernel_size, stride=1, padding=padding)
        self.register_buffer('mask', torch.zeros_like(self.weight)) # ensures mask is stored in state_dict

        if m_type=='A':
            self.mask[:, :, 0:kernel_size//2, :] = 1
            self.mask[:, :, kernel_size//2, 0:kernel_size//2] = 1
        else:
            self.mask[:, :, 0:kernel_size//2, :] = 1
            self.mask[:, :, kernel_size//2, 0:kernel_size//2 + 1] = 1
    
    def forward(self, x):
        self.weight.data *= self.mask
        return super().forward(x)

conv_a = MaskedConv2dBinary('A', 1, 64, 7, 3)
print('Mask A')
print(conv_a.mask[0,:,:,:])

conv_b = MaskedConv2dBinary('B', 64, 64, 7, 3)
print('Mask B')
print(conv_b.mask[0,0,:,:])
</code>
</pre>

<p class="body">Output:</p>
<div class="console">
    <pre style="white-space: pre-line; color: #ffffff">
        Mask A
        tensor([[[1., 1., 1., 1., 1., 1., 1.],
                    [1., 1., 1., 1., 1., 1., 1.],
                    [1., 1., 1., 1., 1., 1., 1.],
                    [1., 1., 1., 0., 0., 0., 0.],
                    [0., 0., 0., 0., 0., 0., 0.],
                    [0., 0., 0., 0., 0., 0., 0.],
                    [0., 0., 0., 0., 0., 0., 0.]]])
        Mask B
        tensor([[1., 1., 1., 1., 1., 1., 1.],
                [1., 1., 1., 1., 1., 1., 1.],
                [1., 1., 1., 1., 1., 1., 1.],
                [1., 1., 1., 1., 0., 0., 0.],
                [0., 0., 0., 0., 0., 0., 0.],
                [0., 0., 0., 0., 0., 0., 0.],
                [0., 0., 0., 0., 0., 0., 0.]])
    </pre>
</div>
                
                 <br>
                 <!-- SECTION -->
                 <h1 class="subhead">3.2 Layer Normalization</h1>
                 <p class="body">
                    Layer normalization is popular in most deep networks to speed up training by transforming hidden layers to be
                    centered around 0 mean and 1 std. Now, after a given masked convolution operation you will have an outputted 
                    tensor with shape (bsize, filters, height, width) = (128, 64, 28, 28). When I first attempted to incorporate layer normalization
                    I figured I should normalize each filter, i.e. calculate the statistics for each 28x28 matrix. <b>Do not do this!</b> Reason being,
                    this violates the auto-regressive property because you are taking the mean and std over all pixels in a filter. Remember, 
                    a given pixel, <span class="math">\(h^l_i\)</span> should not know anything about 
                    <span class="math">\(h^l_{i+1}, h^l_{i+2}, ..., h^l_{n^2}\)</span>. As result, we must normalize over each pixel position 
                    across all 64 filters. This results in <span class="math">\(28^2\)</span> layer norm operations.
                </p>

<pre style="background-color:#f8f9fa; font-size: 11px">
<code>
import torch
from torch.nn import LayerNorm

class LayerNormPixel(LayerNorm):
    def __init__(self, n_filters, affine=True):
        super().__init__([n_filters], elementwise_affine=affine)
    
    def forward(self, x):
        # permute operation returns tensor of shape
        # (bsize, height, width, filters)
        x = super().forward(x.permute(0, 2, 3, 1).contiguous())

        # return tensor is of shape (bsize, filters, height, width)
        return x.permute(0, 3, 1, 2).contiguous()
</code>
</pre>
                <br>
                <!-- SECTION -->
                <h1 class="subhead">3.3 Residual Block</h1>
                <p class="body">
                    Now that we our layer norm and masked convolution classes are implemented, we can create the residual block class.
                </p>

<pre style="background-color:#f8f9fa; font-size: 11px">
<code>
import torch
from torch.nn import ReLU, ModuleList, Module

class ResBlock(Module):
    def __init__(self, m_type, in_channels, out_channels, kernel_size, masked_conv_class):
        super().__init__()
        self.net = ModuleList()
        self.net.append(masked_conv_class(m_type, in_channels, out_channels, 1, 0))
        self.net.append(ReLU())
        p = int((kernel_size - 1)/2)
        self.net.append(masked_conv_class(m_type, in_channels, out_channels, kernel_size, p))
        self.net.append(ReLU())
        self.net.append(masked_conv_class(m_type, in_channels, out_channels, 1, 0))
        self.net.append(LayerNormPixel(out_channels))
        self.net.append(ReLU())
    
    def forward(self, x):
        initial_x = x
        for module in self.net:
            x = module(x)
        
        return initial_x + x
</code>
</pre>
                <br>
                <!-- SECTION -->
                <h1 class="subhead">3.4 Pixel CNN Pytorch</h1>
                <p class="body">
                    Using all created modules above, we can create the Pixel CNN class.
                </p>

<pre style="background-color:#f8f9fa; font-size: 11px">
<code>
class PixelCnn(Module):
    def __init__(
        self, in_dim, channels, kernel_size, layers, filters, 
        dist_size, masked_conv_class):
        super().__init__()
        self.in_dim = in_dim
        self.channels = channels
        self.kernel_size = kernel_size
        self.filters = filters
        self.layers = layers
        self.dist_size = dist_size
        self.mconv = masked_conv_class
        p = int((self.kernel_size - 1)/2)

        self.net = ModuleList()
        self.net.append(self.mconv('A', self.channels, self.filters, self.kernel_size, p))
        self.net.append(LayerNormPixel(self.filters))
        self.net.append(ReLU())
        for _ in range(self.layers-1):
            self.net.append(ResBlock(
                'B', self.filters, self.filters, self.kernel_size, self.mconv))
        self.net.append(self.mconv('B', self.filters, self.filters, 1, 0))
        self.net.append(ReLU())
        self.net.append(self.mconv('B', self.filters, self.dist_size*self.channels, 1, 0))
        
        self.log_softmax = LogSoftmax(dim=1)
        self.loss = NLLLoss(reduction='mean')

    def forward(self, x):
        bsize, _, _, _ = x.shape
        for module in self.net:
            x = module(x)
        
        return self.log_softmax(x.view(bsize, self.dist_size, self.channels, self.in_dim, self.in_dim))

    def get_loss(self, x, y):
        bsize, _, _, _, _ = x.shape
        x1 = x.view((bsize, self.dist_size, self.in_dim*self.in_dim*self.channels))
        y1 = y.view((bsize, self.in_dim*self.in_dim*self.channels))

        return self.loss(x1, y1)

    def generate_samples(self, n, dev, mean, std):
        self.eval()
        samples_in = torch.zeros((n, self.channels, self.in_dim, self.in_dim), dtype=torch.float, device=dev)
        samples_out = torch.zeros((n, self.channels, self.in_dim, self.in_dim), dtype=torch.float, device=dev)

        for row in range(self.in_dim):
            for col in range(self.in_dim):
                for channel in range(self.channels):
                    dist = Categorical(torch.exp(self(samples_in)[:, :, channel, row, col]))
                    s = dist.sample().type(torch.float)
                    samples_in[:, channel, row, col] = (s-mean[channel])/std[channel]
                    samples_out[:, channel, row, col] = s
        
        return samples_out

# two layers so printing area is smaller
model = PixelCnn(28, 1, 7, 2, 64, 2, MaskedConv2dBinary)
print(model)
</code>
</pre>

<p class="body">Output:</p>
<div class="console">
<pre style="color: #ffffff">
PixelCnn(
    (net): ModuleList(
        (0): MaskedConv2dBinary(1, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
        (1): LayerNormPixel(torch.Size([64]), eps=1e-05, elementwise_affine=True)
        (2): ReLU()
        (3): ResBlock(
        (net): ModuleList(
            (0): MaskedConv2dBinary(64, 64, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU()
            (2): MaskedConv2dBinary(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
            (3): ReLU()
            (4): MaskedConv2dBinary(64, 64, kernel_size=(1, 1), stride=(1, 1))
            (5): LayerNormPixel(torch.Size([64]), eps=1e-05, elementwise_affine=True)
            (6): ReLU()
            )
        )
        (4): MaskedConv2dBinary(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (5): ReLU()
        (6): MaskedConv2dBinary(64, 2, kernel_size=(1, 1), stride=(1, 1))
    )
    (log_softmax): LogSoftmax()
    (loss): NLLLoss()
)
</pre>
</div>
<br>
                <!-- SECTION -->
                <h1 class="subhead">3.4.1 Generating Samples</h1>
                <p class="body">
                    In the <code>PixelCnn</code> class you'll notice the <code>generate_samples</code> method. In order to generate
                    <i>n</i> samples we start off of with a tensor of 0s of shape (n, 1, 28, 28). We are going to generate each pixel
                    of the image starting at the top left, moving to the right and then down to the next row of pixels. The generation of 
                    a given pixel requires an entire forward pass through the network. After we sample a given pixel, we update our original 
                    tensor of 0s which is effectively updating the context for the next pixel generation. You'll notice there are two tensors
                    in this method, <code>samples_in</code> and <code>samples_out</code>. <code>samples_in</code> is what goes into the 
                    network and each pixel is standard normal because this is how our model was trained. <code>samples_out</code> 
                    contains the actual generated pixel which in this case will either be a value of 0 or 1.
                </p>

                <!-- SECTION -->
                <h1 class="subhead">3.4.2 Training Script</h1>
                <p class="body">
                    Now that all our modules are implemented we can write the training script, <code>train.py</code>.
                </p>
<pre style="background-color:#f8f9fa; font-size: 11px">
<code>
import numpy as np
import torch
from torch.utils.data import DataLoader
import argparse

import models
from models import PixelCnn
from dataset import Data
from util import init_argparser, show_samples, save_training_plot

def evaluate(test, model):
    loss = 0
    model.eval()

    for batch in test:
        bsize = batch['y'].shape[0]
        loss += model.get_loss(model(batch['x']), batch['y']).item() * bsize
    
    return loss

def train_batch(batch, model, optimizer):
    optimizer.zero_grad()
    preds = model(batch['x'])
    loss = model.get_loss(preds, batch['y'])
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1)
    optimizer.step()

    return loss.item()

def training(train, test, model, optimizer, epochs):
    nlls_train = []
    nlls_test = []

    for epoch in range(1, epochs+1):
        model.train()
        ttl_nll = 0
        for batch in train:
            bsize = batch['y'].shape[0]
            nll = train_batch(batch, model, optimizer)
            ttl_nll += nll * bsize
            nlls_train.append(nll)
        
        nlls_test.append(evaluate(test, model)/len(test.dataset))
        print('epoch '+str(epoch), 'train: '+str(ttl_nll/len(train.dataset)), 'test: '+str(nlls_test[epoch-1]))
    
    return np.array(nlls_train), np.array(nlls_test)

def main(args: argparse.Namespace):
    train_arr, test_arr = Data.read_pickle(args.pickle, 'train'), Data.read_pickle(args.pickle, 'test')
    train = DataLoader(Data(train_arr, args.dev), batch_size=args.bsize, num_workers=args.workers, shuffle=True, collate_fn=Data.collate_fn)
    test = DataLoader(Data(test_arr, args.dev, train.dataset.mean, train.dataset.std), batch_size=args.bsize, num_workers=args.workers,  shuffle=True, collate_fn=Data.collate_fn)
    model = PixelCnn(train.dataset.W, train.dataset.C, args.kernel_size, args.layers, args.filters, args.dist_size, getattr(models, args.conv_class))
    if args.dev=='cuda':
        model.cuda()
    
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
    nlls_train, nlls_test = training(train, test, model, optimizer, args.epochs)
    samples = model.generate_samples(args.n_samples, args.dev, train.dataset.mean, train.dataset.std).cpu().numpy()

    save_training_plot(nlls_train, nlls_test, 'NLL (nats/dim)', args.nll_img_path)
    show_samples(
        samples.reshape(args.n_samples, train.dataset.H, train.dataset.W, train.dataset.C), 
        args.samples_img_path)

if __name__=='__main__':
    main(init_argparser())  
</code>
</pre>
<p class="body">Run:</p>
<div class="console">
<pre style="color: #ffffff">
python train.py --layers 5 --dev cuda --conv_class MaskedConv2dBinary

</pre>
</div>
                <p class="body">
                    You will need to train the model using a GPU. You can import the source code into Google Colab and run the script in there.
                    After the model finishes training, <code>binary_nll.png</code> and <code>binary_samples.png</code> can be found in the 
                    <code>output/</code> directory. Below are those images after training the model for 10 epochs.
                    <br>
                    <div class="rowFigure">
                        <div class="columnFigure">
                            <figure>
                                <img class="fig" src="images/binary_nll.png" width="100%" height="100%">
                                <figcaption class="figcaption">
                                    <span class="figtext">Fig. 5</span> NLL averaged over all pixels for binary MNIST dataset.
                                </figcaption>
                            </figure>
                        </div>
                        <div class="columnFigure">
                            <figure>
                                <img class="fig" src="images/binary_sample.png" width="100%" height="100%">
                                <figcaption class="figcaption">
                                    <span class="figtext">Fig. 6</span> Generated images for binary MNIST dataset.
                                </figcaption>
                            </figure>
                        </div>
                    </div>
                </p>

                <br>
                <!-- SECTION -->
                <h1 class="head">4.0 Pixel CNN Color Images</h1>
                <p class="body">
                    Now we will use the above modules to train a Pixel CNN on colored images. Unfortunately, my repo does not have enough storage 
                    to store the <code>mnist_colored.pkl</code> dataset but you can obtain this dataset from the Berkeley Deep Unsupervised Learning
                    <a class="hyperlinks" href="https://github.com/rll/deepul" target="_blank">repo</a> by unzipping 
                    <code>deepul/homeworks/hw1/data/hw1_data.zip</code>. A given image within in this dataset is of shape (28, 28, 3) and a given pixel takes on a 
                    value between [0, 3]. We will reuse a lot 
                    of the modules we already built, but will need to create a new masked convolution module which is reviewed
                    in the next section.
                </p>

                <br>
                <!-- SECTION -->
                <h1 class="subhead">4.1 Masked Convolution Color Images</h1>
                <p class="body">
                    Below is how the original paper defines Mask A and B for colored images.
                    <figure>
                        <br>
                        <img class="fig" src="images/masks.png" width="40%" height="20%">
                        <figcaption class="figcaption">
                            <span class="figtext">Fig. 7</span> Diagram of the
                            connectivity inside a masked convolution. In the first layer, each
                            of the RGB channels is connected to previous channels and to the
                            context, but is not connected to itself. In subsequent layers, the
                            channels are also connected to themselves. Image taken from 
                            <a class="papers" href="https://arxiv.org/pdf/1601.06759.pdf" target="_blank">Oord et al. 2016</a>.
                        </figcaption>
                        <br>
                    </figure>
                </p>
                <p class="body">In the case of type A, this means:
                    <div class="formula">
                        <br>
                        $$
                        h_{i,R}^1 = f(\pmb{x}_{< i }) \qquad  h_{i,G}^1 = f(\pmb{x}_{< i },x_{i,R}) \qquad h_{i,B}^1 = f(\pmb{x}_{< i },x_{i,R}, x_{i,G})
                        $$
                        <br>
                    </div>
                </p>
                <p class="body">
                    Similar to the toy example in section 3.1.1, assume we have an input image of 3x5x5, kernel size of 3x3 and 
                    3 filters. In this setting the convolution weight matrix, <i>W</i>, would be of shape (3, 3, 3, 3) = (n_filters, in_channels, kernel_height, kernel_width).
                    <i>W</i>[0, :, :, :], <i>W</i>[1, :, :, :] and <i>W</i>[2, :, :, :] are in charge of auto-encoding the R, G and B channels of the input image,
                    respectively. As a result, the mask, <i>M</i>, under type A masked convolution would be as followed:
                    <figure>
                        <br>
                        <img class="fig" src="images/mask-type-A-colored.png" width="70%" height="70%">
                        <figcaption class="figcaption">
                            <span class="figtext">Fig. 8</span> Mask type A tensor for toy example in section 4.1.
                        </figcaption>
                        <br>
                    </figure>
                </p>
                <p class="body">
                    We notice Filter 1 zeros out everything but the context pixels because <span class="math">\(h^1_{i,R}\)</span> is only a function of the context.
                    Filter 2 zeros out everything but the context and <span class="math">\(x_{i,R}\)</span>. Finally, Filter 3 zeros out everything but the context,
                    <span class="math">\(x_{i,R}\)</span>, and <span class="math">\(x_{i,G}\)</span>.
                </p>
                <p class="body">Next we have the mathematical representation of Mask B based off <span class="figtext">Figure 7</span>:
                    <div class="formula">
                        <br>
                        $$
                        h_{i,R}^l = f(\pmb{h}^{l-1}_{< i }, h_{i,R}^{l-1}) \qquad   h_{i,G}^l = f(\pmb{h}^{l-1}_{< i }, h_{i,R}^{l-1}, h_{i,G}^{l-1}) \qquad h_{i,B}^l = f(\pmb{h}^{l-1}_{< i }, h_{i,R}^{l-1}, h_{i,G}^{l-1}, h_{i,B}^{l-1})
                        $$
                        <br>
                    </div>
                </p>
                <p class="body">Where the corresponding image is below.
                <figure>
                    <img class="fig" src="images/mask-type-B-colored.png" width="70%" height="70%">
                    <figcaption class="figcaption">
                        <span class="figtext">Fig. 8</span> Mask type B tensor for toy example in section 4.1.
                    </figcaption>
                    <br>
                </figure>
                </p>

                <!-- SECTION -->
                <h1 class="subhead">4.2 Handling Large Input Channels and Filters</h1>
                <p class="body">
                    Now we need to generalize the above to cases where the number of model filters is greater than 3. Assume we train 
                    our model to have 120 filters at each convolution in the network. As a result, each hidden layer, <span class="math">\(\pmb{h}^l\)</span>, 
                    will have a tensor shape of (bsize, 120, 28, 28). What we do is split the input channels into 3 equal thirds, so 
                    <span class="math">\(\pmb{h}^l\)</span>[:, 0:40, :, :] are <span style="color:red">Red</span> channels, <span class="math">\(\pmb{h}^l\)</span>[:, 40:80, :, :] are <span style="color:green">Green</span> 
                    channels and <span class="math">\(\pmb{h}^l\)</span>[:, 80:, :, :] are <span style="color:navy">Blue</span> channels. Furthermore,
                    Filters 0-39, 40-79 and 80-120 are in charge of the <span style="color:red">Red</span>, <span style="color:green">Green</span> and 
                    <span style="color:navy">Blue</span> channels, respectively. In the of mask type B Filters 0-39, 40-79 and 80-120 would be similar to
                    to Filters 1, 2 and 3 in <span class="figtext">Figure 8</span>, respectively. Below is the Pytorch implementation.
                </p>
<pre style="background-color:#f8f9fa; font-size: 11px">
<code>
class MaskedConv2dColor(MaskedConv2dBinary):
    def __init__(self, m_type, in_channels, out_channels, kernel_size, padding):
        super().__init__(m_type, in_channels, out_channels, kernel_size, padding)
        in_idx = in_channels // 3
        out_idx = out_channels // 3

        if m_type=='A':
            # allow R channels on 2nd third filters
            self.mask[out_idx:out_idx*2, :in_idx, kernel_size//2, kernel_size//2] = 1
            # allow R and G channels on 3rd third filters
            self.mask[out_idx*2:, 0:in_idx*2, kernel_size//2, kernel_size//2 ] = 1
        else:
            # zero out the middle pixel across all input channels and filters
            self.mask[:, :, kernel_size//2, kernel_size//2] = 0
            # allow R channels on the 1st third filters
            self.mask[:out_idx, :in_idx, kernel_size//2, kernel_size//2] = 1
            # allow R and G channels on the 2nd third filters
            self.mask[out_idx:out_idx*2, 0:in_idx*2, kernel_size//2, kernel_size//2] = 1
            # allow R, G and B channels on the 3rd third filters
            self.mask[out_idx*2:, :, kernel_size//2, kernel_size//2] = 1

conv_b = MaskedConv2dColor('B', 120, 120, 7, 3)
print(conv_b.mask.shape) # (n_filters, in_channels, 7, 7)
print(conv_b.mask[50, [10, 50, 90], :, :])
</code>
</pre>
<p class="body">Output:</p>
<div class="console">
<pre style="color: #ffffff">
torch.Size([120, 120, 7, 7])
tensor([[[1., 1., 1., 1., 1., 1., 1.],
            [1., 1., 1., 1., 1., 1., 1.],
            [1., 1., 1., 1., 1., 1., 1.],
            [1., 1., 1., 1., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0.]],

        [[1., 1., 1., 1., 1., 1., 1.],
            [1., 1., 1., 1., 1., 1., 1.],
            [1., 1., 1., 1., 1., 1., 1.],
            [1., 1., 1., 1., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0.]],

        [[1., 1., 1., 1., 1., 1., 1.],
            [1., 1., 1., 1., 1., 1., 1.],
            [1., 1., 1., 1., 1., 1., 1.],
            [1., 1., 1., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0.]]])
</pre>
</div>
<p class="body">Run:</p>
<div class="console">
<pre style="color: #ffffff">
python train.py --pickle data/mnist_colored.pkl --filters 120 --layers 8 --dev cuda --dist_size 4 --conv_class MaskedConv2dColor --nll_img_path output/color_nll.png --samples_img_path output/color_samples.png 

</pre>
</div>
                <div id="disqus_thread"></div>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><hr>
                </div>
            </article>
        </div>
    </div> 
</body>

<!-- mathjax script -->
<script type="text/javascript">
    if (!document.getElementById('mathjaxscript_pelican_#%&#64;#$&#64;#')) {
        var align = "center",
            indent = "0em",
            linebreak = "false";
    
        if (false) {
            align = (screen.width < 768) ? "left" : align;
            indent = (screen.width < 768) ? "0em" : indent;
            linebreak = (screen.width < 768) ? 'true' : linebreak;
        }
    
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%&#64;#$&#64;#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    
        var configscript = document.createElement('script');
        configscript.type = 'text/x-mathjax-config';
        configscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: '"+ align +"'," +
            "    displayIndent: '"+ indent +"'," +
            "    showMathMenu: true," +
            "    messageStyle: 'normal'," +
            "    tex2jax: { " +
            "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        fonts: ['STIX', 'TeX']," +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
            "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
            "    }, " +
            "}); " +
            "if ('default' !== 'default') {" +
                "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                    "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                    "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                    "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                    "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                    "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
                "});" +
                "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                    "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                    "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                    "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                    "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                    "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
                "});" +
            "}";
    
        (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
</script>

<!--- not sure script ??? -->
<script>
        $(document).ready(function(){
          $('[data-toggle="tooltip"]').tooltip(); 
        });
</script>

<!-- disque comments script -->
<script>
    var disqus_config = function () {
    this.page.url = "https://ahernandez105.github.io/TheTransformer.html";
    this.page.identifier ="2";
    };
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://angels-blog-1.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>

<script type="text/javascript">
    window.onload = function(){
        var codeElement = document.getElementById('python_code');
        // Add code mirror class for coloring (default is the theme)
        codeElement.classList.add( 'cm-s-default' );
        var code = codeElement.innerText;

        codeElement.innerHTML = "";

        CodeMirror.runMode(
          code,
          'python',
          codeElement
        );
    };
</script>
</html>