<!doctype html>
<html lang="en">
<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <title>  Pixel CNN | angel's blog </title>
  <link rel="canonical" href="/TheTransformer.html">
  <link rel="stylesheet" href="/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="/theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="/theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="/theme/css/theme.css">
  <link rel="stylesheet" href="/theme/css/md-style.css">
  <link rel="alternate" type="application/atom+xml" title="Full Atom Feed" href="/feeds/all.atom.xml">
  <link rel="alternate" type="application/rss+xml" title="Full RSS Feed" href="/feeds/all.rss.xml">  
  <link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">
  <link rel="stylesheet" href="//codemirror.net/lib/codemirror.css">
  <script src="//codemirror.net/addon/runmode/runmode-standalone.js"></script>
  <script src="//codemirror.net/mode/python/python.js"></script>
  <meta name="description" content="TEXT TEXT">
</head>
<body>
    <header class="header">
        <div class="container">
            <div class="row">
                <div class="col-sm-8">
                    <h1 class="title"><a href="/">angel's blog</a></h1>
                </div>
            </div>    
        </div>
    </header>
    <div class="main">
        <div class="container">
            <h1 class="blogTitle">Pixel CNN</h1>
            <hr>
            <article class="article">
                <header>
                    <ul class="list-inline">
                        <li class="list-inline-item text-muted" title="2019-07-20T00:00:00-04:00"><i class="fa fa-clock-o"></i> ???</li>
                        <li class="list-inline-item text-muted"><i class="fa fa-user-o"></i> Angel C. Hernandez</li>
                    </ul>
                </header>
                <div class="content">
                    <!-- SECTION -->
                    <h1 class="head">1. Introduction </h1>
                    <p class="body">
                        Pixel CNN was proposed in <a class="papers" href="https://arxiv.org/pdf/1601.06759.pdf" target="_blank">Oord et al. 2016</a>
                        and is an auto-regressive generative model. It is effectively an <b>auto encoder</b> that honors the auto-regressive property 
                        using masked convolutions. The data consists of a set of images, 
                        <span class="math">\(\mathcal{D} = \{\pmb{x}^{(t)}\}_{t=1}^T \text{ where } \pmb{x}_i \in \mathbb{R}^{c\times n\times n}\)</span>
                        and <i>T</i> = number of examples, <i>c</i> = channels, and <i>n</i> = height = width of an image.
                        The goal is to learn the joint distribution over all pixels using the chain rule of probability:
                        <div class="formula">
                            <br>
                            $$
                            p(\pmb{x}) = \prod_{i=1}^{n^2} p(x_i|x_1,...,x_{i-1})
                            $$
                            <br>
                        </div>
                    </p>
                    <p class="body">
                        The value <span class="math">\(p(x_i|x_1,...,x_{i-1})\)</span> is the likelihood of the <i>ith</i>
                        pixel, <span class="math">\(x_i\)</span>, given the previous pixels 
                        <span class="math">\(x_1,..., x_{i-1}\)</span>. Pixels are conditioned in a row-by-row pixel-by-pixel 
                        fashion which is highlighted in <span class="figtext">Figure 1.</span>
                        <figure>
                            <br>
                            <img class="fig" src="images/context.png" width="40%" height="20%">
                            <figcaption class="figcaption">
                                <span class="figtext">Fig. 1</span> To generate pixel <span class="math">\(x_i\)</span> one 
                                conditions on all previously generated pixels left and above pixel <span class="math">\(x_i\)</span>.
                                Image taken from <a class="papers" href="https://arxiv.org/pdf/1601.06759.pdf" target_="_blank">Oord et al. 2016</a>.
                            </figcaption>
                            <br>
                        </figure>
                    </p>
                    <p class="body">
                        Moving forward, we will step through how to train a Pixel CNN on both black and white images 
                        and color images using Pytorch. All code reviewed in this post can be found at this GitHub 
                        <a class="hyperlinks" href="https://github.com/ahernandez105/pixelCnn" target_="blank">repo</a>.
                        It should be noted that this blog post was inspired after I completed homework 1 of Berkeley's
                        <a class="hyperlinks" href="https://sites.google.com/view/berkeley-cs294-158-sp20/home">Deep Unsupervised Learning Course.</a>. 
                        While my PixelCNN implementation is not identical to their solution, I do borrow their datasets and some helper methods.
                    </p>
                    <br>
                    <!-- SECTION -->
                    <h1 class="head">2. Loading MNIST Dataset</h1>
                    <p class="body">
                        In the repo you will find a pickled dictionary <code>mnist.pkl</code>. The keys
                        <code>'train'</code> and <code>'test'</code> will map you to the train and test numpy array of images, respectively. 
                        Each image is of shape (28, 28, 1) and takes on value between [0, 255]. To simplify the problem, we will make the images
                        binary by assigning pixels values > 127.5 value <b>1</b> and all other pixels value <b>0</b>. We then will create a Pytorch
                        Dataset and DataLoader where a given batch will be of shape (128, 1, 28, 28) and contains <code>'x'</code> and <code>'y'</code> tensors.
                        The <code>'x'</code> tensor is a batch of images normalized to 0 mean 1 std and the <code>'y'</code> tensor is a batch of 
                        ground truth binary images.
                    </p>
<pre style="background-color:#f8f9fa; font-size: 12px">
<code id="python_code">
import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pickle

class Data(Dataset):
    def __init__(self, array, device='cpu', mean=None, std=None):
        self.N, self.C, self.H, self.W  = array.shape
        self.array = array
        self.device = device
        if mean is None and std is None:
            self.mean = np.mean(self.array, axis=(0,2,3))
            self.std = np.std(self.array, axis=(0,2,3))
        else:
            self.mean = mean
            self.std = std

    def __len__(self):
        return self.N
    
    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.item()
        
        return {
            'x': torch.tensor((self.array[idx] - self.mean)/self.std, dtype=torch.float).to(self.device),
            'y': torch.tensor(self.array[idx], dtype=torch.long).to(self.device)}
    
    @staticmethod
    def collate_fn(batch):
        bsize = len(batch)

        return {
            'x': torch.stack([batch[i]['x'] for i in range(bsize)], dim=0),
            'y': torch.stack([batch[i]['y'] for i in range(bsize)], dim=0)}
    
    @staticmethod
    def read_pickle(fn, dset):
        assert dset=='train' or dset=='test'
        with open(fn, 'rb') as file:
            data = pickle.load(file)[dset]
            data = (data > 127.5).astype('uint8')
            N, H, W, C = data.shape

            return data.reshape(N, C, H, W)

train_arr, test_arr = Data.read_pickle('mnist.pkl', 'train'), Data.read_pickle('mnist.pkl', 'test')
train = DataLoader(Data(train_arr), batch_size=128, shuffle=True, collate_fn=Data.collate_fn)
test = DataLoader(Data(test_arr, mean=train.dataset.mean, std=train.dataset.std), batch_size=128, shuffle=True, collate_fn=Data.collate_fn)

for batch in train:
    print(type(batch))
    print(batch['x'].shape)
    print(batch['y'].shape)
    break
</code>
</pre>
                    <p class="body">Output:</p>
                    <div class="console">
                        <pre style="white-space: pre-line; color: #ffffff">
                        &ltclass 'dict'&gt
                        torch.Size([128, 1, 28, 28])
                        torch.Size([128, 1, 28, 28])
                        </pre>
                    </div>
                    <br>

                    <!-- SECTION -->
                    <h1 class="head">3.0 Pixel CNN Architecture Binary Images</h1>
                    <p class="body">
                        Now we will begin to open up the Pixel CNN architecture. We will review each component in the 
                        architecture which is highlighted in the below figures. Note, the below architecture is almost identical 
                        to the one in the original paper. The only differences are we use a Conv 7x7 in the residual block (as opposed to 3x3),
                        we use 64 convolution filters and include normalization layers.
                    </p>
                    <br>
                    <div class="rowFigure">
                        <div class="columnFigure">
                            <figure>
                                <img class="fig" src="images/pixel-cnn.png" width="80%" height="80%">
                                <figcaption class="figcaption">
                                    <span class="figtext">Fig. 2</span> Pixel CNN architecture<br>used in 
                                    <a class="hyperlinks" href="https://sites.google.com/view/berkeley-cs294-158-sp20/home">Berkeley Deepul</a>. 
                                </figcaption>
                            </figure>
                        </div>
                        <div class="columnFigure">
                            <figure>
                                <img class="fig" src="images/residual-pixel-cnn.png" width="85%" height="85%">
                                <figcaption class="figcaption">
                                    <span class="figtext">Fig. 3</span> Pixel CNN residual block <br>used in 
                                    <a class="hyperlinks" href="https://sites.google.com/view/berkeley-cs294-158-sp20/home">Berkeley Deepul</a>. 
                                </figcaption>
                            </figure>
                        </div>
                    </div>
                    <br>

                    <!-- SECTION -->
                    <h1 class="subhead">3.1 Masked Convolution</h1>
                    <p class="body">
                        As a convolution kernel traverses through the input image, <span class="math">\(\pmb{x}\)</span>, 
                        we need to apply a <b>mask</b> to the kernel to ensure pixels <b>to the left</b> and <b>above</b>
                        the current pixel are only used in the convolution operation. This is referred to as the <b style="color:blue">context</b>
                        and is highlighted in <span class="figtext">Figure 1</span>. Below we will show how to generate
                        the different types of masks, A and B.
                    </p>

                    <!-- SECTION -->
                    <h1 class="subhead">3.1.1 Mask Type A Single Channel</h1>
                    <p class="body">
                        One neat thing about this architecture is the width and height of the input image will be maintained 
                        across all layers within the network. For simplicity, let's assume an input image of shape 5x5 with one channel and kernel
                        size of 3x3. Next, use the below formula to determine how much we need to pad our input image to maintain the same width/height:
                        <div class="formula">
                            <br>
                            \begin{aligned}
                            W_{\text{out}} &= \frac{W_{\text{in}} \ - \text{ kernel_size } + \ 2P}{\text{stride}} + 1\\
                            5 &= \frac{5 - 3 + 2P}{1} + 1 \\
                            P &= 1
                            \end{aligned}
                            <br>
                        </div>
                    </p>
                    <p class="body">
                        Next, we need to apply a <b>mask</b> to our kernel to honor the auto-regressive property. <span class="figtext">Figure 4</span> shows 
                        the masked convolution operation for this toy example.
                        <figure>
                            <br>
                            <img class="fig" src="images/mask-type-A.png" width="90%" height="70%">
                            <figcaption class="figcaption">
                                <span class="figtext">Fig. 4</span> Masked type A convolution at two different points in time.
                            </figcaption>
                            <br>
                        </figure>
                    </p>
                </div>
            </article>
        </div>
    </div> 
</body>
<!-- mathjax script -->
<script type="text/javascript">
    if (!document.getElementById('mathjaxscript_pelican_#%&#64;#$&#64;#')) {
        var align = "center",
            indent = "0em",
            linebreak = "false";
    
        if (false) {
            align = (screen.width < 768) ? "left" : align;
            indent = (screen.width < 768) ? "0em" : indent;
            linebreak = (screen.width < 768) ? 'true' : linebreak;
        }
    
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%&#64;#$&#64;#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    
        var configscript = document.createElement('script');
        configscript.type = 'text/x-mathjax-config';
        configscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: '"+ align +"'," +
            "    displayIndent: '"+ indent +"'," +
            "    showMathMenu: true," +
            "    messageStyle: 'normal'," +
            "    tex2jax: { " +
            "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        fonts: ['STIX', 'TeX']," +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
            "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
            "    }, " +
            "}); " +
            "if ('default' !== 'default') {" +
                "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                    "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                    "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                    "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                    "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                    "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
                "});" +
                "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                    "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                    "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                    "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                    "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                    "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
                "});" +
            "}";
    
        (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
</script>
<!--- not sure script ??? -->
<script>
        $(document).ready(function(){
          $('[data-toggle="tooltip"]').tooltip(); 
        });
</script>
<!-- disque comments script -->
<script>
    var disqus_config = function () {
    this.page.url = "https://ahernandez105.github.io/TheTransformer.html";
    this.page.identifier ="2";
    };
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://angels-blog-1.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>

<script type="text/javascript">
    window.onload = function(){
        var codeElement = document.getElementById('python_code');
        // Add code mirror class for coloring (default is the theme)
        codeElement.classList.add( 'cm-s-default' );
        var code = codeElement.innerText;

        codeElement.innerHTML = "";

        CodeMirror.runMode(
          code,
          'python',
          codeElement
        );
    };
</script>
</html>